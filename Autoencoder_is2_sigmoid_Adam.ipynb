{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoder_is2_sigmoid_Adam.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s1230038/autoencoder/blob/master/Autoencoder_is2_sigmoid_Adam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "42k_7hqZp7yx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rm *txt *h5 *png *zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d5NaL7in5Jc1",
        "colab_type": "code",
        "outputId": "fe78093f-185f-4cb1-82df-a2b8f9a278c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 922
        }
      },
      "cell_type": "code",
      "source": [
        "# Simple Autoencoder using Other Loss Function\n",
        "# Original: https://elix-tech.github.io/ja/2016/07/17/autoencoder.html\n",
        "# https://colab.research.google.com/drive/1Z_d8APkMUDwXDQIg3OI7E13vH8IZhusM?authuser=1#scrollTo=WmBfOis_mWCH\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras import backend as K\n",
        "from keras import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import losses\n",
        "from tensorflow.python import debug as tf_debug\n",
        "from itertools import product\n",
        "from google.colab import files\n",
        "from keras import optimizers\n",
        "\n",
        "# imititing mean_squared_error():\n",
        "# 青イルカ P.57では二乗誤差の総和を使用しているが、これをKerasで実装する場合は平均二乗誤差（mean_squared_error)\n",
        "# をloss に渡す必要がある。なぜなら、Kerasはミニバッチでfit()を計算するからである。青イルカ P.27参照。\n",
        "# 𝒙 ̂_𝑛 :y_pred,  𝒙_𝑛 : y_true, because x_n is training data which means label.\n",
        "def i_divergence1(y_true, y_pred):\n",
        "  y_true = K.clip(y_true, K.epsilon(), 1)\n",
        "  y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
        "  return K.mean(y_pred * K.log(y_pred / y_true) - y_pred + y_true, axis=-1)\n",
        "\n",
        "def i_divergence2(y_true, y_pred):\n",
        "  y_true = K.clip(y_true, K.epsilon(), 1)\n",
        "  y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
        "  return K.mean(y_true * K.log(y_true / y_pred) - y_true + y_pred, axis=-1)\n",
        "\n",
        "def is_divergence1(y_true, y_pred):\n",
        "  y_true = K.clip(y_true, K.epsilon(), 1)\n",
        "  y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
        "  return K.mean( (y_pred / y_true) - K.log(y_pred / y_true) - 1, axis=-1)\n",
        "\n",
        "def is_divergence2(y_true, y_pred):\n",
        "  y_true = K.clip(y_true, K.epsilon(), 1)\n",
        "  y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
        "  return K.mean( (y_true / y_pred) - K.log(y_true / y_pred) - 1, axis=-1)\n",
        "\n",
        "# https://qiita.com/hiroyuki827/items/213146d551a6e2227810\n",
        "def plot_history_loss(np_loss, np_vloss, name):\n",
        "    # Plot the loss in the history\n",
        "    fig, axL = plt.subplots(figsize=(8,6), dpi=500) # グラフの表示準備\n",
        "    axL.plot(np_loss, label=\"loss for training\")\n",
        "    axL.plot(np_vloss, label=\"loss for validation\")\n",
        "    axL.set_title('model loss: ' + name)\n",
        "    axL.set_xlabel('epoch')\n",
        "    axL.set_ylabel('loss')\n",
        "    axL.legend(loc='upper right')\n",
        "    return fig\n",
        "\n",
        "\n",
        "epochs = 50\n",
        "# Download MNIST and standardize, learning\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# we will restrict domain of definition of the input data to the below expression with normalization of the input.\n",
        "x_train = x_train.astype('float32') / 255. # 画像データは0から1の実数値を取るように規格化\n",
        "x_test = x_test.astype('float32') / 255.   # {0,1}の二値ではなく実数値であることに注意\n",
        "# x_trainは (60000, 28, 28) という形をしていますが、784次元の入力になるように (60000, 784) に変形\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "# hyper parameter combination\n",
        "'''\n",
        "lossfs = [losses.mean_squared_error, i_divergence1, i_divergence2, is_divergence1, is_divergence2]\n",
        "acts = [\"relu\", \"sigmoid\"]\n",
        "opzs = [\"SGD\", \"RMSprop\", \"Adagrad\", \"Adadelta\", \"Adam\", \"Adamax\", \"Nadam\"]\n",
        "'''\n",
        "lossfs = [is_divergence2]\n",
        "acts = [\"sigmoid\"]\n",
        "list1  = [0.0017000000000000003]\n",
        "list2  = [0.0]\n",
        "# ------------------------------------------\n",
        "for loss, dact, lr, dc in product(lossfs, acts, list1, list2):\n",
        "  optimizer = optimizers.Adam(lr=lr, decay=dc)\n",
        "  file_prefix = loss.__name__ + '_' + dact + '_' + optimizer.__class__.__name__ + '_'  + str(lr) + '_' + str(dc) + '_'\n",
        "  print(\"start: \" + file_prefix )\n",
        "  encoding_dim = 32\n",
        "  input_img = Input(shape=(784,))\n",
        "  encoded = Dense(encoding_dim, activation='relu')(input_img) \n",
        "  decoded = Dense(784, activation=dact)(encoded)\n",
        "  autoencoder = Model(inputs=input_img, outputs=decoded) # https://medium.com/@zhuixiyou/userwarning-update-your-model-call-to-the-keras-2-api-8a6a5955daac\n",
        "  # autoencoderでは、教師データにラベルを使わないためaccuracyの計算は不要。\n",
        "  autoencoder.compile(optimizer=optimizer, loss=loss)\n",
        "  # ------------------------------------------\n",
        "  fit = autoencoder.fit(x_train, x_train,\n",
        "                  epochs=epochs,\n",
        "                  batch_size=256,\n",
        "                  shuffle=True,\n",
        "                  verbose=0,\n",
        "                  validation_data=(x_test, x_test))\n",
        "\n",
        "  # lossのCSVファイルの保存\n",
        "  loss_his = fit.history['loss']\n",
        "  vloss_his = fit.history['val_loss']\n",
        "  np_loss = np.array(loss_his)\n",
        "  np_vloss = np.array(vloss_his)\n",
        "  np.savetxt(file_prefix + \"loss_history.txt\",     np_loss,  delimiter=\",\")\n",
        "  np.savetxt(file_prefix + \"val_loss_history.txt\", np_vloss, delimiter=\",\")\n",
        "  \n",
        "  # グラフの保存\n",
        "  fig = plot_history_loss(np_loss, np_vloss, loss.__name__)\n",
        "  fig.savefig(file_prefix + \"loss_history.png\")\n",
        "  plt.close()\n",
        "  \n",
        "  # 学習した重みを保存\n",
        "  autoencoder.save_weights(file_prefix + 'autoencoder.h5')\n",
        "  \n",
        "  autoencoder.reset_states()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start: is_divergence2_sigmoid_Adam_0.0017000000000000003_0.0_\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1af9d9c023c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m                   \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                   \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                   validation_data=(x_test, x_test))\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m   \u001b[0;31m# lossのCSVファイルの保存\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "mVwTIp0j2BUE",
        "colab_type": "code",
        "outputId": "274f1fbe-0775-4237-dc8e-f7810f55ea34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "for loss, dact, lr, dc in product(lossfs, acts, list1, list2):\n",
        "  optimizer = optimizers.Adam(lr=lr, decay=dc)\n",
        "  file_prefix = loss.__name__ + '_' + dact + '_' + optimizer.__class__.__name__ + '_'  + str(lr) + '_' + str(dc) + '_'\n",
        "  print(\"start: \" + file_prefix )\n",
        "  encoding_dim = 32\n",
        "  input_img = Input(shape=(784,))\n",
        "  encoded = Dense(encoding_dim, activation='relu')(input_img) \n",
        "  decoded = Dense(784, activation=dact)(encoded)\n",
        "  autoencoder = Model(inputs=input_img, outputs=decoded) # https://medium.com/@zhuixiyou/userwarning-update-your-model-call-to-the-keras-2-api-8a6a5955daac\n",
        "  # autoencoderでは、教師データにラベルを使わないためaccuracyの計算は不要。\n",
        "  autoencoder.compile(optimizer=optimizer, loss=loss)\n",
        "  # ------------------------------------------\n",
        "  # 保存した重みを読み込み\n",
        "  autoencoder.load_weights(file_prefix + 'autoencoder.h5')\n",
        "  # x_test -> x_train, y_test->y_train\n",
        "  decoded_imgs = autoencoder.predict(x_train)\n",
        "  # 0-9を表示する\n",
        "  plt.figure(figsize=(20, 4))\n",
        "  label = 0\n",
        "  for i in range(1000):\n",
        "      if label > 10:\n",
        "          break\n",
        "      elif label != y_train[i]:\n",
        "          continue\n",
        "      # オリジナルのテスト画像を表示\n",
        "      ax = plt.subplot(2, 10, label+1)\n",
        "      plt.imshow(x_train[i].reshape(28, 28))\n",
        "      plt.gray()\n",
        "      ax.get_xaxis().set_visible(False)\n",
        "      ax.get_yaxis().set_visible(False)\n",
        "\n",
        "      # 変換された画像を表示\n",
        "      ax = plt.subplot(2, 10, label+1+10)\n",
        "      plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "      plt.gray()\n",
        "      ax.get_xaxis().set_visible(False)\n",
        "      ax.get_yaxis().set_visible(False)\n",
        "      \n",
        "      label+=1\n",
        "  #  plt.show() #show()するとファイルに保存できないことに注意。\n",
        "  plt.savefig(file_prefix + 'numbers.png')\n",
        "  autoencoder.reset_states()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: \n",
            "This call to matplotlib.use() has no effect because the backend has already\n",
            "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
            "or matplotlib.backends is imported for the first time.\n",
            "\n",
            "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 657, in launch_instance\n",
            "    app.initialize(argv)\n",
            "  File \"<decorator-gen-121>\", line 2, in initialize\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 87, in catch_config_error\n",
            "    return method(app, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 462, in initialize\n",
            "    self.init_gui_pylab()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 403, in init_gui_pylab\n",
            "    InteractiveShellApp.init_gui_pylab(self)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/shellapp.py\", line 213, in init_gui_pylab\n",
            "    r = enable(key)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n",
            "    pt.activate_matplotlib(backend)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n",
            "    matplotlib.pyplot.switch_backend(backend)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n",
            "    matplotlib.use(newbackend, warn=False, force=True)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n",
            "    reload(sys.modules['matplotlib.backends'])\n",
            "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
            "    _bootstrap._exec(spec, module)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n",
            "    line for line in traceback.format_stack()\n",
            "\n",
            "\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "start: is_divergence2_sigmoid_Adam_0.0017000000000000003_0.0_\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABGoAAADjCAYAAADdR/IFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGgAAAAAAAE/woAYAAAAAAMAT/w8qXoPLF7qHEgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7ff82b0034a8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "LK23VO2zjflG",
        "colab_type": "code",
        "outputId": "206ba126-414c-4e97-fed0-d068f5cadb8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1835
        }
      },
      "cell_type": "code",
      "source": [
        "# 保存した重みのダウンロード\n",
        "from google.colab import files\n",
        "!zip -r h5.zip *.h5\n",
        "!zip -r png.zip *.png\n",
        "!zip -r txt.zip *.txt\n",
        "files.download('h5.zip')\n",
        "files.download('png.zip')\n",
        "files.download('txt.zip')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: is_divergence1_relu_SGD_0.0_0.005_autoencoder.h5 (deflated 15%)\n",
            "  adding: is_divergence1_relu_SGD_0.0_0.0_autoencoder.h5 (deflated 16%)\n",
            "  adding: is_divergence1_relu_SGD_0.01_0.005_autoencoder.h5 (deflated 13%)\n",
            "  adding: is_divergence1_relu_SGD_0.01_0.0_autoencoder.h5 (deflated 13%)\n",
            "  adding: is_divergence1_relu_SGD_0.02_0.005_autoencoder.h5 (deflated 13%)\n",
            "  adding: is_divergence1_relu_SGD_0.02_0.0_autoencoder.h5 (deflated 13%)\n",
            "  adding: is_divergence1_relu_SGD_0.03_0.005_autoencoder.h5 (deflated 13%)\n",
            "  adding: is_divergence1_relu_SGD_0.03_0.0_autoencoder.h5 (deflated 13%)\n",
            "  adding: is_divergence1_relu_SGD_0.04_0.005_autoencoder.h5 (deflated 13%)\n",
            "  adding: is_divergence1_relu_SGD_0.04_0.0_autoencoder.h5 (deflated 13%)\n",
            "  adding: is_divergence1_relu_SGD_0.05_0.005_autoencoder.h5 (deflated 13%)\n",
            "  adding: is_divergence1_relu_SGD_0.05_0.0_autoencoder.h5 (deflated 13%)\n",
            "  adding: is_divergence1_relu_SGD_0.06_0.005_autoencoder.h5 (deflated 13%)\n",
            "  adding: is_divergence1_relu_SGD_0.06_0.0_autoencoder.h5 (deflated 13%)\n",
            "  adding: is_divergence1_relu_SGD_0.07_0.005_autoencoder.h5 (deflated 13%)\n",
            "  adding: is_divergence1_relu_SGD_0.07_0.0_autoencoder.h5 (deflated 13%)\n",
            "  adding: is_divergence1_relu_SGD_0.08_0.005_autoencoder.h5 (deflated 13%)\n",
            "  adding: is_divergence1_relu_SGD_0.08_0.0_autoencoder.h5 (deflated 13%)\n",
            "  adding: is_divergence1_relu_SGD_0.09_0.005_autoencoder.h5 (deflated 12%)\n",
            "  adding: is_divergence1_relu_SGD_0.09_0.0_autoencoder.h5 (deflated 13%)\n",
            "  adding: is_divergence1_relu_SGD_0.0_0.005_loss_history.png (deflated 35%)\n",
            "  adding: is_divergence1_relu_SGD_0.0_0.005_numbers.png (deflated 12%)\n",
            "  adding: is_divergence1_relu_SGD_0.0_0.0_loss_history.png (deflated 34%)\n",
            "  adding: is_divergence1_relu_SGD_0.0_0.0_numbers.png (deflated 12%)\n",
            "  adding: is_divergence1_relu_SGD_0.01_0.005_loss_history.png (deflated 32%)\n",
            "  adding: is_divergence1_relu_SGD_0.01_0.005_numbers.png (deflated 22%)\n",
            "  adding: is_divergence1_relu_SGD_0.01_0.0_loss_history.png (deflated 30%)\n",
            "  adding: is_divergence1_relu_SGD_0.01_0.0_numbers.png (deflated 25%)\n",
            "  adding: is_divergence1_relu_SGD_0.02_0.005_loss_history.png (deflated 30%)\n",
            "  adding: is_divergence1_relu_SGD_0.02_0.005_numbers.png (deflated 25%)\n",
            "  adding: is_divergence1_relu_SGD_0.02_0.0_loss_history.png (deflated 33%)\n",
            "  adding: is_divergence1_relu_SGD_0.02_0.0_numbers.png (deflated 25%)\n",
            "  adding: is_divergence1_relu_SGD_0.03_0.005_loss_history.png (deflated 31%)\n",
            "  adding: is_divergence1_relu_SGD_0.03_0.005_numbers.png (deflated 25%)\n",
            "  adding: is_divergence1_relu_SGD_0.03_0.0_loss_history.png (deflated 31%)\n",
            "  adding: is_divergence1_relu_SGD_0.03_0.0_numbers.png (deflated 25%)\n",
            "  adding: is_divergence1_relu_SGD_0.04_0.005_loss_history.png (deflated 32%)\n",
            "  adding: is_divergence1_relu_SGD_0.04_0.005_numbers.png (deflated 25%)\n",
            "  adding: is_divergence1_relu_SGD_0.04_0.0_loss_history.png (deflated 31%)\n",
            "  adding: is_divergence1_relu_SGD_0.04_0.0_numbers.png (deflated 25%)\n",
            "  adding: is_divergence1_relu_SGD_0.05_0.005_loss_history.png (deflated 32%)\n",
            "  adding: is_divergence1_relu_SGD_0.05_0.005_numbers.png (deflated 25%)\n",
            "  adding: is_divergence1_relu_SGD_0.05_0.0_loss_history.png (deflated 32%)\n",
            "  adding: is_divergence1_relu_SGD_0.05_0.0_numbers.png (deflated 25%)\n",
            "  adding: is_divergence1_relu_SGD_0.06_0.005_loss_history.png (deflated 32%)\n",
            "  adding: is_divergence1_relu_SGD_0.06_0.005_numbers.png (deflated 25%)\n",
            "  adding: is_divergence1_relu_SGD_0.06_0.0_loss_history.png (deflated 32%)\n",
            "  adding: is_divergence1_relu_SGD_0.06_0.0_numbers.png (deflated 25%)\n",
            "  adding: is_divergence1_relu_SGD_0.07_0.005_loss_history.png (deflated 32%)\n",
            "  adding: is_divergence1_relu_SGD_0.07_0.005_numbers.png (deflated 25%)\n",
            "  adding: is_divergence1_relu_SGD_0.07_0.0_loss_history.png (deflated 32%)\n",
            "  adding: is_divergence1_relu_SGD_0.07_0.0_numbers.png (deflated 25%)\n",
            "  adding: is_divergence1_relu_SGD_0.08_0.005_loss_history.png (deflated 33%)\n",
            "  adding: is_divergence1_relu_SGD_0.08_0.005_numbers.png (deflated 25%)\n",
            "  adding: is_divergence1_relu_SGD_0.08_0.0_loss_history.png (deflated 31%)\n",
            "  adding: is_divergence1_relu_SGD_0.08_0.0_numbers.png (deflated 25%)\n",
            "  adding: is_divergence1_relu_SGD_0.09_0.005_loss_history.png (deflated 30%)\n",
            "  adding: is_divergence1_relu_SGD_0.09_0.005_numbers.png (deflated 25%)\n",
            "  adding: is_divergence1_relu_SGD_0.09_0.0_loss_history.png (deflated 32%)\n",
            "  adding: is_divergence1_relu_SGD_0.09_0.0_numbers.png (deflated 25%)\n",
            "  adding: is_divergence1_relu_SGD_0.0_0.005_loss_history.txt (deflated 78%)\n",
            "  adding: is_divergence1_relu_SGD_0.0_0.005_val_loss_history.txt (deflated 97%)\n",
            "  adding: is_divergence1_relu_SGD_0.0_0.0_loss_history.txt (deflated 79%)\n",
            "  adding: is_divergence1_relu_SGD_0.0_0.0_val_loss_history.txt (deflated 97%)\n",
            "  adding: is_divergence1_relu_SGD_0.01_0.005_loss_history.txt (deflated 79%)\n",
            "  adding: is_divergence1_relu_SGD_0.01_0.005_val_loss_history.txt (deflated 97%)\n",
            "  adding: is_divergence1_relu_SGD_0.01_0.0_loss_history.txt (deflated 72%)\n",
            "  adding: is_divergence1_relu_SGD_0.01_0.0_val_loss_history.txt (deflated 97%)\n",
            "  adding: is_divergence1_relu_SGD_0.02_0.005_loss_history.txt (deflated 74%)\n",
            "  adding: is_divergence1_relu_SGD_0.02_0.005_val_loss_history.txt (deflated 97%)\n",
            "  adding: is_divergence1_relu_SGD_0.02_0.0_loss_history.txt (deflated 71%)\n",
            "  adding: is_divergence1_relu_SGD_0.02_0.0_val_loss_history.txt (deflated 97%)\n",
            "  adding: is_divergence1_relu_SGD_0.03_0.005_loss_history.txt (deflated 73%)\n",
            "  adding: is_divergence1_relu_SGD_0.03_0.005_val_loss_history.txt (deflated 97%)\n",
            "  adding: is_divergence1_relu_SGD_0.03_0.0_loss_history.txt (deflated 73%)\n",
            "  adding: is_divergence1_relu_SGD_0.03_0.0_val_loss_history.txt (deflated 97%)\n",
            "  adding: is_divergence1_relu_SGD_0.04_0.005_loss_history.txt (deflated 72%)\n",
            "  adding: is_divergence1_relu_SGD_0.04_0.005_val_loss_history.txt (deflated 97%)\n",
            "  adding: is_divergence1_relu_SGD_0.04_0.0_loss_history.txt (deflated 72%)\n",
            "  adding: is_divergence1_relu_SGD_0.04_0.0_val_loss_history.txt (deflated 97%)\n",
            "  adding: is_divergence1_relu_SGD_0.05_0.005_loss_history.txt (deflated 70%)\n",
            "  adding: is_divergence1_relu_SGD_0.05_0.005_val_loss_history.txt (deflated 97%)\n",
            "  adding: is_divergence1_relu_SGD_0.05_0.0_loss_history.txt (deflated 73%)\n",
            "  adding: is_divergence1_relu_SGD_0.05_0.0_val_loss_history.txt (deflated 97%)\n",
            "  adding: is_divergence1_relu_SGD_0.06_0.005_loss_history.txt (deflated 72%)\n",
            "  adding: is_divergence1_relu_SGD_0.06_0.005_val_loss_history.txt (deflated 97%)\n",
            "  adding: is_divergence1_relu_SGD_0.06_0.0_loss_history.txt (deflated 72%)\n",
            "  adding: is_divergence1_relu_SGD_0.06_0.0_val_loss_history.txt (deflated 97%)\n",
            "  adding: is_divergence1_relu_SGD_0.07_0.005_loss_history.txt (deflated 72%)\n",
            "  adding: is_divergence1_relu_SGD_0.07_0.005_val_loss_history.txt (deflated 93%)\n",
            "  adding: is_divergence1_relu_SGD_0.07_0.0_loss_history.txt (deflated 72%)\n",
            "  adding: is_divergence1_relu_SGD_0.07_0.0_val_loss_history.txt (deflated 97%)\n",
            "  adding: is_divergence1_relu_SGD_0.08_0.005_loss_history.txt (deflated 73%)\n",
            "  adding: is_divergence1_relu_SGD_0.08_0.005_val_loss_history.txt (deflated 97%)\n",
            "  adding: is_divergence1_relu_SGD_0.08_0.0_loss_history.txt (deflated 73%)\n",
            "  adding: is_divergence1_relu_SGD_0.08_0.0_val_loss_history.txt (deflated 97%)\n",
            "  adding: is_divergence1_relu_SGD_0.09_0.005_loss_history.txt (deflated 73%)\n",
            "  adding: is_divergence1_relu_SGD_0.09_0.005_val_loss_history.txt (deflated 97%)\n",
            "  adding: is_divergence1_relu_SGD_0.09_0.0_loss_history.txt (deflated 72%)\n",
            "  adding: is_divergence1_relu_SGD_0.09_0.0_val_loss_history.txt (deflated 97%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}