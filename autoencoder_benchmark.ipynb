{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autoencoder_benchmark.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s1230038/autoencoder/blob/master/autoencoder_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "G0mWuegD6cyK",
        "colab_type": "code",
        "outputId": "d9d87f71-73e4-4469-eb9f-459919a9e0cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7365
        }
      },
      "cell_type": "code",
      "source": [
        "# https://elix-tech.github.io/ja/2016/07/17/autoencoder.html\n",
        "# adding https://gertjanvandenburg.com/blog/autoencoder/\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras import backend as K\n",
        "\n",
        "# adding https://gertjanvandenburg.com/blog/autoencoder/\n",
        "input_img = Input(shape=(28*28,))\n",
        "l1 = Dense(50, activation='tanh')(input_img)\n",
        "l2 = Dense(50, activation='tanh')(l1)\n",
        "l3 = Dense(2, activation='linear')(l2)\n",
        "l4 = Dense(50, activation='tanh')(l3)\n",
        "l5 = Dense(50, activation='tanh')(l4)\n",
        "out = Dense(28*28, activation='relu')(l5)\n",
        "autoencoder = Model(inputs=input_img, outputs=out)\n",
        "autoencoder.compile(optimizer='adam', metrics=['accuracy'], loss='mean_squared_error')\n",
        "\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255. # 画像データは0から1の値を取るように規格化\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "# x_trainは (60000, 28, 28) という形をしていますが、784次元の入力になるように (60000, 784) に変形\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                nb_epoch=200,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/200\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 0.0672 - acc: 0.0075 - val_loss: 0.0585 - val_acc: 0.0113\n",
            "Epoch 2/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0558 - acc: 0.0125 - val_loss: 0.0536 - val_acc: 0.0118\n",
            "Epoch 3/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0522 - acc: 0.0114 - val_loss: 0.0502 - val_acc: 0.0126\n",
            "Epoch 4/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0493 - acc: 0.0134 - val_loss: 0.0482 - val_acc: 0.0141\n",
            "Epoch 5/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0475 - acc: 0.0118 - val_loss: 0.0467 - val_acc: 0.0101\n",
            "Epoch 6/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0462 - acc: 0.0110 - val_loss: 0.0455 - val_acc: 0.0081\n",
            "Epoch 7/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0451 - acc: 0.0104 - val_loss: 0.0445 - val_acc: 0.0095\n",
            "Epoch 8/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0443 - acc: 0.0103 - val_loss: 0.0439 - val_acc: 0.0105\n",
            "Epoch 9/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0437 - acc: 0.0104 - val_loss: 0.0434 - val_acc: 0.0093\n",
            "Epoch 10/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0432 - acc: 0.0108 - val_loss: 0.0430 - val_acc: 0.0141\n",
            "Epoch 11/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0427 - acc: 0.0104 - val_loss: 0.0427 - val_acc: 0.0089\n",
            "Epoch 12/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0423 - acc: 0.0099 - val_loss: 0.0424 - val_acc: 0.0113\n",
            "Epoch 13/200\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.0420 - acc: 0.0101 - val_loss: 0.0421 - val_acc: 0.0123\n",
            "Epoch 14/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0417 - acc: 0.0103 - val_loss: 0.0418 - val_acc: 0.0094\n",
            "Epoch 15/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0414 - acc: 0.0096 - val_loss: 0.0416 - val_acc: 0.0102\n",
            "Epoch 16/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0412 - acc: 0.0094 - val_loss: 0.0414 - val_acc: 0.0106\n",
            "Epoch 17/200\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.0410 - acc: 0.0098 - val_loss: 0.0412 - val_acc: 0.0097\n",
            "Epoch 18/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0408 - acc: 0.0103 - val_loss: 0.0411 - val_acc: 0.0103\n",
            "Epoch 19/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0406 - acc: 0.0104 - val_loss: 0.0411 - val_acc: 0.0116\n",
            "Epoch 20/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0404 - acc: 0.0101 - val_loss: 0.0408 - val_acc: 0.0109\n",
            "Epoch 21/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0403 - acc: 0.0097 - val_loss: 0.0407 - val_acc: 0.0120\n",
            "Epoch 22/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0401 - acc: 0.0104 - val_loss: 0.0405 - val_acc: 0.0132\n",
            "Epoch 23/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0400 - acc: 0.0100 - val_loss: 0.0405 - val_acc: 0.0099\n",
            "Epoch 24/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0399 - acc: 0.0106 - val_loss: 0.0404 - val_acc: 0.0121\n",
            "Epoch 25/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0397 - acc: 0.0101 - val_loss: 0.0403 - val_acc: 0.0122\n",
            "Epoch 26/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0396 - acc: 0.0102 - val_loss: 0.0403 - val_acc: 0.0090\n",
            "Epoch 27/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0395 - acc: 0.0106 - val_loss: 0.0401 - val_acc: 0.0112\n",
            "Epoch 28/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0394 - acc: 0.0103 - val_loss: 0.0400 - val_acc: 0.0093\n",
            "Epoch 29/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0393 - acc: 0.0102 - val_loss: 0.0399 - val_acc: 0.0100\n",
            "Epoch 30/200\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.0392 - acc: 0.0099 - val_loss: 0.0399 - val_acc: 0.0092\n",
            "Epoch 31/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0391 - acc: 0.0098 - val_loss: 0.0399 - val_acc: 0.0100\n",
            "Epoch 32/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0390 - acc: 0.0104 - val_loss: 0.0397 - val_acc: 0.0090\n",
            "Epoch 33/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0389 - acc: 0.0101 - val_loss: 0.0397 - val_acc: 0.0111\n",
            "Epoch 34/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0388 - acc: 0.0099 - val_loss: 0.0396 - val_acc: 0.0093\n",
            "Epoch 35/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0387 - acc: 0.0099 - val_loss: 0.0396 - val_acc: 0.0104\n",
            "Epoch 36/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0386 - acc: 0.0100 - val_loss: 0.0396 - val_acc: 0.0109\n",
            "Epoch 37/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0385 - acc: 0.0097 - val_loss: 0.0395 - val_acc: 0.0095\n",
            "Epoch 38/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0385 - acc: 0.0096 - val_loss: 0.0394 - val_acc: 0.0108\n",
            "Epoch 39/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0384 - acc: 0.0094 - val_loss: 0.0394 - val_acc: 0.0108\n",
            "Epoch 40/200\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.0383 - acc: 0.0101 - val_loss: 0.0392 - val_acc: 0.0090\n",
            "Epoch 41/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0383 - acc: 0.0090 - val_loss: 0.0392 - val_acc: 0.0102\n",
            "Epoch 42/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0382 - acc: 0.0096 - val_loss: 0.0393 - val_acc: 0.0087\n",
            "Epoch 43/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0381 - acc: 0.0096 - val_loss: 0.0392 - val_acc: 0.0114\n",
            "Epoch 44/200\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.0381 - acc: 0.0096 - val_loss: 0.0391 - val_acc: 0.0107\n",
            "Epoch 45/200\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.0380 - acc: 0.0102 - val_loss: 0.0391 - val_acc: 0.0113\n",
            "Epoch 46/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0380 - acc: 0.0096 - val_loss: 0.0391 - val_acc: 0.0110\n",
            "Epoch 47/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0379 - acc: 0.0100 - val_loss: 0.0390 - val_acc: 0.0094\n",
            "Epoch 48/200\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.0379 - acc: 0.0103 - val_loss: 0.0389 - val_acc: 0.0113\n",
            "Epoch 49/200\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.0378 - acc: 0.0100 - val_loss: 0.0389 - val_acc: 0.0119\n",
            "Epoch 50/200\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.0378 - acc: 0.0097 - val_loss: 0.0389 - val_acc: 0.0106\n",
            "Epoch 51/200\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.0377 - acc: 0.0099 - val_loss: 0.0389 - val_acc: 0.0097\n",
            "Epoch 52/200\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.0377 - acc: 0.0104 - val_loss: 0.0389 - val_acc: 0.0107\n",
            "Epoch 53/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0376 - acc: 0.0098 - val_loss: 0.0389 - val_acc: 0.0128\n",
            "Epoch 54/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0376 - acc: 0.0097 - val_loss: 0.0388 - val_acc: 0.0103\n",
            "Epoch 55/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0376 - acc: 0.0100 - val_loss: 0.0388 - val_acc: 0.0096\n",
            "Epoch 56/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0375 - acc: 0.0102 - val_loss: 0.0387 - val_acc: 0.0118\n",
            "Epoch 57/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0375 - acc: 0.0098 - val_loss: 0.0387 - val_acc: 0.0111\n",
            "Epoch 58/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0374 - acc: 0.0098 - val_loss: 0.0387 - val_acc: 0.0138\n",
            "Epoch 59/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0374 - acc: 0.0104 - val_loss: 0.0386 - val_acc: 0.0092\n",
            "Epoch 60/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0374 - acc: 0.0100 - val_loss: 0.0386 - val_acc: 0.0111\n",
            "Epoch 61/200\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0373 - acc: 0.0098 - val_loss: 0.0386 - val_acc: 0.0104\n",
            "Epoch 62/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0373 - acc: 0.0103 - val_loss: 0.0386 - val_acc: 0.0111\n",
            "Epoch 63/200\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.0372 - acc: 0.0096 - val_loss: 0.0386 - val_acc: 0.0142\n",
            "Epoch 64/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0372 - acc: 0.0097 - val_loss: 0.0385 - val_acc: 0.0108\n",
            "Epoch 65/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0372 - acc: 0.0102 - val_loss: 0.0386 - val_acc: 0.0126\n",
            "Epoch 66/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0372 - acc: 0.0097 - val_loss: 0.0386 - val_acc: 0.0130\n",
            "Epoch 67/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0372 - acc: 0.0103 - val_loss: 0.0385 - val_acc: 0.0113\n",
            "Epoch 68/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0371 - acc: 0.0099 - val_loss: 0.0385 - val_acc: 0.0120\n",
            "Epoch 69/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0371 - acc: 0.0105 - val_loss: 0.0385 - val_acc: 0.0123\n",
            "Epoch 70/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0371 - acc: 0.0102 - val_loss: 0.0385 - val_acc: 0.0116\n",
            "Epoch 71/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0370 - acc: 0.0103 - val_loss: 0.0384 - val_acc: 0.0142\n",
            "Epoch 72/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0370 - acc: 0.0106 - val_loss: 0.0384 - val_acc: 0.0114\n",
            "Epoch 73/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0370 - acc: 0.0102 - val_loss: 0.0384 - val_acc: 0.0117\n",
            "Epoch 74/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0369 - acc: 0.0104 - val_loss: 0.0384 - val_acc: 0.0120\n",
            "Epoch 75/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0369 - acc: 0.0100 - val_loss: 0.0384 - val_acc: 0.0135\n",
            "Epoch 76/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0369 - acc: 0.0098 - val_loss: 0.0383 - val_acc: 0.0145\n",
            "Epoch 77/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0369 - acc: 0.0108 - val_loss: 0.0383 - val_acc: 0.0109\n",
            "Epoch 78/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0369 - acc: 0.0107 - val_loss: 0.0384 - val_acc: 0.0117\n",
            "Epoch 79/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0368 - acc: 0.0106 - val_loss: 0.0383 - val_acc: 0.0130\n",
            "Epoch 80/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0368 - acc: 0.0096 - val_loss: 0.0385 - val_acc: 0.0118\n",
            "Epoch 81/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0368 - acc: 0.0096 - val_loss: 0.0383 - val_acc: 0.0124\n",
            "Epoch 82/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0368 - acc: 0.0093 - val_loss: 0.0383 - val_acc: 0.0122\n",
            "Epoch 83/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0367 - acc: 0.0103 - val_loss: 0.0382 - val_acc: 0.0121\n",
            "Epoch 84/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0367 - acc: 0.0098 - val_loss: 0.0383 - val_acc: 0.0138\n",
            "Epoch 85/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0367 - acc: 0.0103 - val_loss: 0.0383 - val_acc: 0.0102\n",
            "Epoch 86/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0367 - acc: 0.0100 - val_loss: 0.0383 - val_acc: 0.0124\n",
            "Epoch 87/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0367 - acc: 0.0099 - val_loss: 0.0382 - val_acc: 0.0106\n",
            "Epoch 88/200\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.0366 - acc: 0.0104 - val_loss: 0.0382 - val_acc: 0.0121\n",
            "Epoch 89/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0366 - acc: 0.0102 - val_loss: 0.0383 - val_acc: 0.0127\n",
            "Epoch 90/200\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.0366 - acc: 0.0097 - val_loss: 0.0382 - val_acc: 0.0127\n",
            "Epoch 91/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0366 - acc: 0.0094 - val_loss: 0.0382 - val_acc: 0.0124\n",
            "Epoch 92/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0366 - acc: 0.0100 - val_loss: 0.0382 - val_acc: 0.0114\n",
            "Epoch 93/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0366 - acc: 0.0098 - val_loss: 0.0382 - val_acc: 0.0101\n",
            "Epoch 94/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0365 - acc: 0.0095 - val_loss: 0.0381 - val_acc: 0.0124\n",
            "Epoch 95/200\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.0365 - acc: 0.0097 - val_loss: 0.0381 - val_acc: 0.0125\n",
            "Epoch 96/200\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.0365 - acc: 0.0103 - val_loss: 0.0381 - val_acc: 0.0113\n",
            "Epoch 97/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0365 - acc: 0.0098 - val_loss: 0.0381 - val_acc: 0.0127\n",
            "Epoch 98/200\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.0365 - acc: 0.0098 - val_loss: 0.0381 - val_acc: 0.0128\n",
            "Epoch 99/200\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.0365 - acc: 0.0100 - val_loss: 0.0381 - val_acc: 0.0120\n",
            "Epoch 100/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0364 - acc: 0.0100 - val_loss: 0.0380 - val_acc: 0.0125\n",
            "Epoch 101/200\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.0364 - acc: 0.0096 - val_loss: 0.0381 - val_acc: 0.0104\n",
            "Epoch 102/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0364 - acc: 0.0095 - val_loss: 0.0381 - val_acc: 0.0124\n",
            "Epoch 103/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0364 - acc: 0.0094 - val_loss: 0.0380 - val_acc: 0.0126\n",
            "Epoch 104/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0363 - acc: 0.0100 - val_loss: 0.0381 - val_acc: 0.0131\n",
            "Epoch 105/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0363 - acc: 0.0094 - val_loss: 0.0381 - val_acc: 0.0106\n",
            "Epoch 106/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0363 - acc: 0.0094 - val_loss: 0.0380 - val_acc: 0.0112\n",
            "Epoch 107/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0363 - acc: 0.0101 - val_loss: 0.0381 - val_acc: 0.0116\n",
            "Epoch 108/200\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0363 - acc: 0.0095 - val_loss: 0.0380 - val_acc: 0.0121\n",
            "Epoch 109/200\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.0363 - acc: 0.0097 - val_loss: 0.0380 - val_acc: 0.0121\n",
            "Epoch 110/200\n",
            "60000/60000 [==============================] - 2s 40us/step - loss: 0.0363 - acc: 0.0095 - val_loss: 0.0380 - val_acc: 0.0100\n",
            "Epoch 111/200\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 0.0363 - acc: 0.0101 - val_loss: 0.0379 - val_acc: 0.0121\n",
            "Epoch 112/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0362 - acc: 0.0096 - val_loss: 0.0379 - val_acc: 0.0128\n",
            "Epoch 113/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0362 - acc: 0.0098 - val_loss: 0.0379 - val_acc: 0.0109\n",
            "Epoch 114/200\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0362 - acc: 0.0092 - val_loss: 0.0379 - val_acc: 0.0116\n",
            "Epoch 115/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0362 - acc: 0.0092 - val_loss: 0.0379 - val_acc: 0.0120\n",
            "Epoch 116/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0362 - acc: 0.0096 - val_loss: 0.0379 - val_acc: 0.0123\n",
            "Epoch 117/200\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.0362 - acc: 0.0100 - val_loss: 0.0380 - val_acc: 0.0116\n",
            "Epoch 118/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0362 - acc: 0.0093 - val_loss: 0.0379 - val_acc: 0.0121\n",
            "Epoch 119/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0361 - acc: 0.0095 - val_loss: 0.0380 - val_acc: 0.0131\n",
            "Epoch 120/200\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.0361 - acc: 0.0092 - val_loss: 0.0379 - val_acc: 0.0088\n",
            "Epoch 121/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0361 - acc: 0.0092 - val_loss: 0.0379 - val_acc: 0.0136\n",
            "Epoch 122/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0361 - acc: 0.0096 - val_loss: 0.0379 - val_acc: 0.0100\n",
            "Epoch 123/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0361 - acc: 0.0089 - val_loss: 0.0379 - val_acc: 0.0111\n",
            "Epoch 124/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0361 - acc: 0.0101 - val_loss: 0.0380 - val_acc: 0.0106\n",
            "Epoch 125/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0361 - acc: 0.0094 - val_loss: 0.0378 - val_acc: 0.0115\n",
            "Epoch 126/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0361 - acc: 0.0091 - val_loss: 0.0379 - val_acc: 0.0111\n",
            "Epoch 127/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0361 - acc: 0.0098 - val_loss: 0.0379 - val_acc: 0.0127\n",
            "Epoch 128/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0360 - acc: 0.0098 - val_loss: 0.0378 - val_acc: 0.0118\n",
            "Epoch 129/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0360 - acc: 0.0099 - val_loss: 0.0380 - val_acc: 0.0121\n",
            "Epoch 130/200\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0360 - acc: 0.0094 - val_loss: 0.0379 - val_acc: 0.0104\n",
            "Epoch 131/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0360 - acc: 0.0098 - val_loss: 0.0379 - val_acc: 0.0114\n",
            "Epoch 132/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0360 - acc: 0.0095 - val_loss: 0.0379 - val_acc: 0.0120\n",
            "Epoch 133/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0360 - acc: 0.0089 - val_loss: 0.0379 - val_acc: 0.0136\n",
            "Epoch 134/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0360 - acc: 0.0089 - val_loss: 0.0378 - val_acc: 0.0110\n",
            "Epoch 135/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0360 - acc: 0.0091 - val_loss: 0.0379 - val_acc: 0.0098\n",
            "Epoch 136/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0360 - acc: 0.0096 - val_loss: 0.0379 - val_acc: 0.0108\n",
            "Epoch 137/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0360 - acc: 0.0094 - val_loss: 0.0378 - val_acc: 0.0106\n",
            "Epoch 138/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0359 - acc: 0.0094 - val_loss: 0.0378 - val_acc: 0.0142\n",
            "Epoch 139/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0359 - acc: 0.0097 - val_loss: 0.0379 - val_acc: 0.0107\n",
            "Epoch 140/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0359 - acc: 0.0095 - val_loss: 0.0379 - val_acc: 0.0097\n",
            "Epoch 141/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0359 - acc: 0.0099 - val_loss: 0.0378 - val_acc: 0.0127\n",
            "Epoch 142/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0359 - acc: 0.0093 - val_loss: 0.0379 - val_acc: 0.0129\n",
            "Epoch 143/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0359 - acc: 0.0100 - val_loss: 0.0378 - val_acc: 0.0114\n",
            "Epoch 144/200\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.0359 - acc: 0.0097 - val_loss: 0.0378 - val_acc: 0.0116\n",
            "Epoch 145/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0359 - acc: 0.0091 - val_loss: 0.0378 - val_acc: 0.0094\n",
            "Epoch 146/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0359 - acc: 0.0090 - val_loss: 0.0378 - val_acc: 0.0122\n",
            "Epoch 147/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0359 - acc: 0.0091 - val_loss: 0.0379 - val_acc: 0.0123\n",
            "Epoch 148/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0358 - acc: 0.0092 - val_loss: 0.0378 - val_acc: 0.0109\n",
            "Epoch 149/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0358 - acc: 0.0099 - val_loss: 0.0378 - val_acc: 0.0116\n",
            "Epoch 150/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0359 - acc: 0.0092 - val_loss: 0.0378 - val_acc: 0.0101\n",
            "Epoch 151/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0358 - acc: 0.0103 - val_loss: 0.0378 - val_acc: 0.0117\n",
            "Epoch 152/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0358 - acc: 0.0089 - val_loss: 0.0377 - val_acc: 0.0100\n",
            "Epoch 153/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0358 - acc: 0.0089 - val_loss: 0.0378 - val_acc: 0.0109\n",
            "Epoch 154/200\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0358 - acc: 0.0090 - val_loss: 0.0378 - val_acc: 0.0131\n",
            "Epoch 155/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0358 - acc: 0.0095 - val_loss: 0.0379 - val_acc: 0.0121\n",
            "Epoch 156/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0358 - acc: 0.0086 - val_loss: 0.0378 - val_acc: 0.0119\n",
            "Epoch 157/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0358 - acc: 0.0095 - val_loss: 0.0378 - val_acc: 0.0104\n",
            "Epoch 158/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0358 - acc: 0.0096 - val_loss: 0.0378 - val_acc: 0.0103\n",
            "Epoch 159/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0358 - acc: 0.0093 - val_loss: 0.0378 - val_acc: 0.0093\n",
            "Epoch 160/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0358 - acc: 0.0089 - val_loss: 0.0378 - val_acc: 0.0108\n",
            "Epoch 161/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0358 - acc: 0.0096 - val_loss: 0.0377 - val_acc: 0.0112\n",
            "Epoch 162/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0358 - acc: 0.0088 - val_loss: 0.0378 - val_acc: 0.0113\n",
            "Epoch 163/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0358 - acc: 0.0094 - val_loss: 0.0378 - val_acc: 0.0114\n",
            "Epoch 164/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0357 - acc: 0.0092 - val_loss: 0.0379 - val_acc: 0.0116\n",
            "Epoch 165/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0357 - acc: 0.0099 - val_loss: 0.0378 - val_acc: 0.0104\n",
            "Epoch 166/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0357 - acc: 0.0097 - val_loss: 0.0378 - val_acc: 0.0108\n",
            "Epoch 167/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0357 - acc: 0.0095 - val_loss: 0.0378 - val_acc: 0.0108\n",
            "Epoch 168/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0357 - acc: 0.0095 - val_loss: 0.0378 - val_acc: 0.0104\n",
            "Epoch 169/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0357 - acc: 0.0098 - val_loss: 0.0378 - val_acc: 0.0117\n",
            "Epoch 170/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0357 - acc: 0.0093 - val_loss: 0.0378 - val_acc: 0.0116\n",
            "Epoch 171/200\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.0357 - acc: 0.0099 - val_loss: 0.0377 - val_acc: 0.0110\n",
            "Epoch 172/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0357 - acc: 0.0100 - val_loss: 0.0377 - val_acc: 0.0114\n",
            "Epoch 173/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0357 - acc: 0.0094 - val_loss: 0.0377 - val_acc: 0.0115\n",
            "Epoch 174/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0357 - acc: 0.0092 - val_loss: 0.0377 - val_acc: 0.0115\n",
            "Epoch 175/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0357 - acc: 0.0095 - val_loss: 0.0377 - val_acc: 0.0115\n",
            "Epoch 176/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0356 - acc: 0.0090 - val_loss: 0.0377 - val_acc: 0.0111\n",
            "Epoch 177/200\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0356 - acc: 0.0101 - val_loss: 0.0378 - val_acc: 0.0100\n",
            "Epoch 178/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0356 - acc: 0.0090 - val_loss: 0.0378 - val_acc: 0.0108\n",
            "Epoch 179/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0356 - acc: 0.0098 - val_loss: 0.0377 - val_acc: 0.0108\n",
            "Epoch 180/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0356 - acc: 0.0091 - val_loss: 0.0378 - val_acc: 0.0089\n",
            "Epoch 181/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0356 - acc: 0.0092 - val_loss: 0.0378 - val_acc: 0.0109\n",
            "Epoch 182/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0356 - acc: 0.0087 - val_loss: 0.0378 - val_acc: 0.0109\n",
            "Epoch 183/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0356 - acc: 0.0097 - val_loss: 0.0377 - val_acc: 0.0107\n",
            "Epoch 184/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0356 - acc: 0.0090 - val_loss: 0.0378 - val_acc: 0.0102\n",
            "Epoch 185/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0356 - acc: 0.0093 - val_loss: 0.0377 - val_acc: 0.0089\n",
            "Epoch 186/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0356 - acc: 0.0094 - val_loss: 0.0377 - val_acc: 0.0108\n",
            "Epoch 187/200\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.0356 - acc: 0.0094 - val_loss: 0.0377 - val_acc: 0.0119\n",
            "Epoch 188/200\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.0356 - acc: 0.0099 - val_loss: 0.0377 - val_acc: 0.0114\n",
            "Epoch 189/200\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.0356 - acc: 0.0097 - val_loss: 0.0378 - val_acc: 0.0098\n",
            "Epoch 190/200\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.0356 - acc: 0.0091 - val_loss: 0.0378 - val_acc: 0.0100\n",
            "Epoch 191/200\n",
            "60000/60000 [==============================] - 2s 38us/step - loss: 0.0356 - acc: 0.0095 - val_loss: 0.0378 - val_acc: 0.0088\n",
            "Epoch 192/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0356 - acc: 0.0096 - val_loss: 0.0377 - val_acc: 0.0108\n",
            "Epoch 193/200\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0356 - acc: 0.0095 - val_loss: 0.0377 - val_acc: 0.0107\n",
            "Epoch 194/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0356 - acc: 0.0098 - val_loss: 0.0378 - val_acc: 0.0095\n",
            "Epoch 195/200\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0356 - acc: 0.0100 - val_loss: 0.0377 - val_acc: 0.0142\n",
            "Epoch 196/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0355 - acc: 0.0093 - val_loss: 0.0377 - val_acc: 0.0100\n",
            "Epoch 197/200\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0355 - acc: 0.0095 - val_loss: 0.0377 - val_acc: 0.0105\n",
            "Epoch 198/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0355 - acc: 0.0097 - val_loss: 0.0377 - val_acc: 0.0119\n",
            "Epoch 199/200\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.0355 - acc: 0.0098 - val_loss: 0.0377 - val_acc: 0.0112\n",
            "Epoch 200/200\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0355 - acc: 0.0092 - val_loss: 0.0377 - val_acc: 0.0115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe34ab58f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "_UBTsMfR6tb6",
        "colab_type": "code",
        "outputId": "2ccb88c3-616e-47ce-af9b-8b3a0ddf1591",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 学習した重みを保存したり、読み込みたい場合\n",
        "autoencoder.save_weights('autoencoder.h5')\n",
        "autoencoder.load_weights('autoencoder.h5')\n",
        "\n",
        "# テスト画像を変換\n",
        "decoded_imgs = autoencoder.predict(x_test)\n",
        "\n",
        "# 何個表示するか\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # オリジナルのテスト画像を表示\n",
        "    ax = plt.subplot(2, n, i+1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # 変換された画像を表示\n",
        "    ax = plt.subplot(2, n, i+1+n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABGoAAADjCAYAAADdR/IFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAMuL/AYYovruaqXTKAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fe34b75ada0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "6pP4V8ttJS5v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('autoencoder.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}