{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AutoencoderInTensorFlow.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"VKIlmjn6LyB8","colab_type":"code","outputId":"2de94c34-3613-4336-82bd-79eab7871a18","executionInfo":{"status":"ok","timestamp":1544513920118,"user_tz":-540,"elapsed":3418,"user":{"displayName":"DL user","photoUrl":"","userId":"07468112820899660153"}},"colab":{"base_uri":"https://localhost:8080/","height":181}},"cell_type":"code","source":["!ls -l"],"execution_count":0,"outputs":[{"output_type":"stream","text":["total 41096\n","-rw-r--r-- 1 root root 14438535 Dec 11 07:37 autoencoder.zip\n","drwxr-xr-x 2 root root    12288 Dec 11 07:17 images\n","-rw-r--r-- 1 root root   760452 Dec 11 07:17 latent_default.csv\n","drwxr-xr-x 2 root root     4096 Dec 11 07:08 logs\n","-rwxr-xr-x 1 root root 16117632 Jul 15  2017 ngrok\n","-rw-r--r-- 1 root root  5363700 Dec 11 06:51 ngrok-stable-linux-amd64.zip\n","-rw-r--r-- 1 root root  5363700 Dec 11 06:52 ngrok-stable-linux-amd64.zip.1\n","drwxr-xr-x 1 root root     4096 Dec  7 17:32 sample_data\n"],"name":"stdout"}]},{"metadata":{"id":"Ynz-vlK_Lrpk","colab_type":"code","outputId":"52a23589-c55d-4fbf-b139-364d8278f1d9","executionInfo":{"status":"ok","timestamp":1544513903280,"user_tz":-540,"elapsed":73805,"user":{"displayName":"DL user","photoUrl":"","userId":"07468112820899660153"}},"colab":{"base_uri":"https://localhost:8080/","height":3819}},"cell_type":"code","source":["from google.colab import files\n","!zip -r autoencoder.zip images latent_default.csv logs\n","files.download('autoencoder.zip')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["  adding: images/ (stored 0%)\n","  adding: images/output_184000.jpg (deflated 0%)\n","  adding: images/output_162000.jpg (deflated 0%)\n","  adding: images/output_117000.jpg (deflated 0%)\n","  adding: images/output_136000.jpg (deflated 0%)\n","  adding: images/output_186000.jpg (deflated 0%)\n","  adding: images/output_134000.jpg (deflated 0%)\n","  adding: images/output_141000.jpg (deflated 0%)\n","  adding: images/output_173000.jpg (deflated 0%)\n","  adding: images/output_040000.jpg (deflated 0%)\n","  adding: images/output_015000.jpg (deflated 0%)\n","  adding: images/output_132000.jpg (deflated 0%)\n","  adding: images/output_042000.jpg (deflated 0%)\n","  adding: images/output_043000.jpg (deflated 0%)\n","  adding: images/output_084000.jpg (deflated 0%)\n","  adding: images/output_089000.jpg (deflated 0%)\n","  adding: images/output_120000.jpg (deflated 0%)\n","  adding: images/input.jpg (deflated 2%)\n","  adding: images/output_013000.jpg (deflated 0%)\n","  adding: images/output_048000.jpg (deflated 0%)\n","  adding: images/output_056000.jpg (deflated 0%)\n","  adding: images/output_126000.jpg (deflated 0%)\n","  adding: images/output_052000.jpg (deflated 0%)\n","  adding: images/output_169000.jpg (deflated 0%)\n","  adding: images/output_191000.jpg (deflated 0%)\n","  adding: images/output_166000.jpg (deflated 0%)\n","  adding: images/output_177000.jpg (deflated 0%)\n","  adding: images/output_003000.jpg (deflated 0%)\n","  adding: images/output_075000.jpg (deflated 0%)\n","  adding: images/output_069000.jpg (deflated 0%)\n","  adding: images/output_028000.jpg (deflated 0%)\n","  adding: images/output_172000.jpg (deflated 0%)\n","  adding: images/output_050000.jpg (deflated 0%)\n","  adding: images/output_001000.jpg (deflated 0%)\n","  adding: images/output_196000.jpg (deflated 0%)\n","  adding: images/output_097000.jpg (deflated 0%)\n","  adding: images/output_139000.jpg (deflated 0%)\n","  adding: images/output_155000.jpg (deflated 0%)\n","  adding: images/output_157000.jpg (deflated 0%)\n","  adding: images/output_140000.jpg (deflated 0%)\n","  adding: images/output_168000.jpg (deflated 0%)\n","  adding: images/output_079000.jpg (deflated 0%)\n","  adding: images/output_091000.jpg (deflated 0%)\n","  adding: images/output_133000.jpg (deflated 0%)\n","  adding: images/output_152000.jpg (deflated 0%)\n","  adding: images/output_021000.jpg (deflated 0%)\n","  adding: images/output_107000.jpg (deflated 0%)\n","  adding: images/output_020000.jpg (deflated 0%)\n","  adding: images/output_039000.jpg (deflated 0%)\n","  adding: images/output_065000.jpg (deflated 0%)\n","  adding: images/output_183000.jpg (deflated 0%)\n","  adding: images/output_053000.jpg (deflated 0%)\n","  adding: images/output_073000.jpg (deflated 0%)\n","  adding: images/output_082000.jpg (deflated 0%)\n","  adding: images/output_113000.jpg (deflated 0%)\n","  adding: images/output_137000.jpg (deflated 0%)\n","  adding: images/output_086000.jpg (deflated 0%)\n","  adding: images/output_018000.jpg (deflated 0%)\n","  adding: images/output_170000.jpg (deflated 0%)\n","  adding: images/output_077000.jpg (deflated 0%)\n","  adding: images/output_083000.jpg (deflated 0%)\n","  adding: images/output_146000.jpg (deflated 0%)\n","  adding: images/output_034000.jpg (deflated 0%)\n","  adding: images/output_031000.jpg (deflated 0%)\n","  adding: images/output_145000.jpg (deflated 0%)\n","  adding: images/output_144000.jpg (deflated 0%)\n","  adding: images/output_088000.jpg (deflated 0%)\n","  adding: images/output_119000.jpg (deflated 0%)\n","  adding: images/output_049000.jpg (deflated 0%)\n","  adding: images/output_005000.jpg (deflated 0%)\n","  adding: images/output_102000.jpg (deflated 0%)\n","  adding: images/output_122000.jpg (deflated 0%)\n","  adding: images/output_199000.jpg (deflated 0%)\n","  adding: images/output_171000.jpg (deflated 0%)\n","  adding: images/output_072000.jpg (deflated 0%)\n","  adding: images/output_142000.jpg (deflated 0%)\n","  adding: images/output_121000.jpg (deflated 0%)\n","  adding: images/output_070000.jpg (deflated 0%)\n","  adding: images/output_071000.jpg (deflated 0%)\n","  adding: images/output_096000.jpg (deflated 0%)\n","  adding: images/output_105000.jpg (deflated 0%)\n","  adding: images/output_098000.jpg (deflated 0%)\n","  adding: images/output_000000.jpg (deflated 4%)\n","  adding: images/output_111000.jpg (deflated 0%)\n","  adding: images/output_055000.jpg (deflated 0%)\n","  adding: images/output_030000.jpg (deflated 0%)\n","  adding: images/output_130000.jpg (deflated 0%)\n","  adding: images/output_093000.jpg (deflated 0%)\n","  adding: images/output_023000.jpg (deflated 0%)\n","  adding: images/output_002000.jpg (deflated 0%)\n","  adding: images/output_081000.jpg (deflated 0%)\n","  adding: images/output_011000.jpg (deflated 0%)\n","  adding: images/output_051000.jpg (deflated 0%)\n","  adding: images/output_190000.jpg (deflated 0%)\n","  adding: images/output_118000.jpg (deflated 0%)\n","  adding: images/output_035000.jpg (deflated 0%)\n","  adding: images/output_010000.jpg (deflated 0%)\n","  adding: images/output_006000.jpg (deflated 0%)\n","  adding: images/output_154000.jpg (deflated 0%)\n","  adding: images/output_085000.jpg (deflated 0%)\n","  adding: images/output_007000.jpg (deflated 0%)\n","  adding: images/output_017000.jpg (deflated 0%)\n","  adding: images/output_041000.jpg (deflated 0%)\n","  adding: images/output_197000.jpg (deflated 0%)\n","  adding: images/output_045000.jpg (deflated 0%)\n","  adding: images/output_009000.jpg (deflated 0%)\n","  adding: images/output_104000.jpg (deflated 0%)\n","  adding: images/output_198000.jpg (deflated 0%)\n","  adding: images/output_019000.jpg (deflated 0%)\n","  adding: images/output_061000.jpg (deflated 0%)\n","  adding: images/output_025000.jpg (deflated 0%)\n","  adding: images/output_135000.jpg (deflated 0%)\n","  adding: images/output_038000.jpg (deflated 0%)\n","  adding: images/output_159000.jpg (deflated 0%)\n","  adding: images/output_193000.jpg (deflated 0%)\n","  adding: images/output_087000.jpg (deflated 0%)\n","  adding: images/output_032000.jpg (deflated 0%)\n","  adding: images/output_161000.jpg (deflated 0%)\n","  adding: images/output_138000.jpg (deflated 0%)\n","  adding: images/output_024000.jpg (deflated 0%)\n","  adding: images/output_150000.jpg (deflated 0%)\n","  adding: images/output_101000.jpg (deflated 0%)\n","  adding: images/output_124000.jpg (deflated 0%)\n","  adding: images/output_108000.jpg (deflated 0%)\n","  adding: images/output_153000.jpg (deflated 0%)\n","  adding: images/output_189000.jpg (deflated 0%)\n","  adding: images/output_176000.jpg (deflated 0%)\n","  adding: images/output_046000.jpg (deflated 0%)\n","  adding: images/output_067000.jpg (deflated 0%)\n","  adding: images/output_163000.jpg (deflated 0%)\n","  adding: images/output_027000.jpg (deflated 0%)\n","  adding: images/output_044000.jpg (deflated 0%)\n","  adding: images/output_078000.jpg (deflated 0%)\n","  adding: images/output_022000.jpg (deflated 0%)\n","  adding: images/output_129000.jpg (deflated 0%)\n","  adding: images/output_158000.jpg (deflated 0%)\n","  adding: images/output_059000.jpg (deflated 0%)\n","  adding: images/output_029000.jpg (deflated 0%)\n","  adding: images/output_192000.jpg (deflated 0%)\n","  adding: images/output_057000.jpg (deflated 0%)\n","  adding: images/output_188000.jpg (deflated 0%)\n","  adding: images/output_092000.jpg (deflated 0%)\n","  adding: images/output_187000.jpg (deflated 0%)\n","  adding: images/output_066000.jpg (deflated 0%)\n","  adding: images/output_014000.jpg (deflated 0%)\n","  adding: images/output_036000.jpg (deflated 0%)\n","  adding: images/output_110000.jpg (deflated 0%)\n","  adding: images/output_164000.jpg (deflated 0%)\n","  adding: images/output_180000.jpg (deflated 0%)\n","  adding: images/output_174000.jpg (deflated 0%)\n","  adding: images/output_149000.jpg (deflated 0%)\n","  adding: images/output_178000.jpg (deflated 0%)\n","  adding: images/output_064000.jpg (deflated 0%)\n","  adding: images/output_054000.jpg (deflated 0%)\n","  adding: images/output_033000.jpg (deflated 0%)\n","  adding: images/output_194000.jpg (deflated 0%)\n","  adding: images/output_167000.jpg (deflated 0%)\n","  adding: images/output_112000.jpg (deflated 0%)\n","  adding: images/output_063000.jpg (deflated 0%)\n","  adding: images/output_106000.jpg (deflated 0%)\n","  adding: images/output_090000.jpg (deflated 0%)\n","  adding: images/output_147000.jpg (deflated 0%)\n","  adding: images/output_114000.jpg (deflated 0%)\n","  adding: images/output_185000.jpg (deflated 0%)\n","  adding: images/output_058000.jpg (deflated 0%)\n","  adding: images/output_160000.jpg (deflated 0%)\n","  adding: images/output_099000.jpg (deflated 0%)\n","  adding: images/output_094000.jpg (deflated 0%)\n","  adding: images/output_128000.jpg (deflated 0%)\n","  adding: images/output_103000.jpg (deflated 0%)\n","  adding: images/output_115000.jpg (deflated 0%)\n","  adding: images/output_109000.jpg (deflated 0%)\n","  adding: images/output_004000.jpg (deflated 0%)\n","  adding: images/output_116000.jpg (deflated 0%)\n","  adding: images/output_026000.jpg (deflated 0%)\n","  adding: images/output_165000.jpg (deflated 0%)\n","  adding: images/output_047000.jpg (deflated 0%)\n","  adding: images/output_151000.jpg (deflated 0%)\n","  adding: images/output_095000.jpg (deflated 0%)\n","  adding: images/output_143000.jpg (deflated 0%)\n","  adding: images/output_080000.jpg (deflated 0%)\n","  adding: images/output_179000.jpg (deflated 0%)\n","  adding: images/output_076000.jpg (deflated 0%)\n","  adding: images/output_074000.jpg (deflated 0%)\n","  adding: images/output_148000.jpg (deflated 0%)\n","  adding: images/output_123000.jpg (deflated 0%)\n","  adding: images/output_037000.jpg (deflated 0%)\n","  adding: images/output_068000.jpg (deflated 0%)\n","  adding: images/output_156000.jpg (deflated 0%)\n","  adding: images/output_016000.jpg (deflated 0%)\n","  adding: images/output_131000.jpg (deflated 0%)\n","  adding: images/output_062000.jpg (deflated 0%)\n","  adding: images/output_195000.jpg (deflated 0%)\n","  adding: images/output_008000.jpg (deflated 0%)\n","  adding: images/output_100000.jpg (deflated 0%)\n","  adding: images/output_127000.jpg (deflated 0%)\n","  adding: images/output_012000.jpg (deflated 0%)\n","  adding: images/output_181000.jpg (deflated 0%)\n","  adding: images/output_060000.jpg (deflated 0%)\n","  adding: images/output_175000.jpg (deflated 0%)\n","  adding: images/output_182000.jpg (deflated 0%)\n","  adding: images/output_200000.jpg (deflated 0%)\n","  adding: images/output_125000.jpg (deflated 0%)\n","  adding: latent_default.csv (deflated 70%)\n","  adding: logs/ (stored 0%)\n","  adding: logs/events.out.tfevents.1544512109.d403cbbfd321 (deflated 1%)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/lib/python3.6/threading.py:864: ResourceWarning: unclosed <socket.socket fd=88, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=0, laddr=('::', 16348, 0, 0)>\n","  self._target(*self._args, **self._kwargs)\n"],"name":"stderr"}]},{"metadata":{"id":"yoG5F3YMpWSf","colab_type":"code","outputId":"bdba5cbb-9f52-49b1-e8b4-d8d729f00b62","executionInfo":{"status":"ok","timestamp":1544511174897,"user_tz":-540,"elapsed":19499,"user":{"displayName":"DL user","photoUrl":"","userId":"07468112820899660153"}},"colab":{"base_uri":"https://localhost:8080/","height":290}},"cell_type":"code","source":["# Setup ngrok and run TensorBoard on Colab\n","# https://www.dlology.com/blog/quick-guide-to-run-tensorboard-in-google-colab/\n","!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip ngrok-stable-linux-amd64.zip\n","\n","LOG_DIR = './logs'\n","get_ipython().system_raw(\n","    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n","    .format(LOG_DIR)\n",")\n","\n","get_ipython().system_raw('./ngrok http 6006 &')\n","\n","! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2018-12-11 06:52:35--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","Resolving bin.equinox.io (bin.equinox.io)... 34.199.255.1, 34.206.9.96, 34.204.22.7, ...\n","Connecting to bin.equinox.io (bin.equinox.io)|34.199.255.1|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5363700 (5.1M) [application/octet-stream]\n","Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n","\n","ngrok-stable-linux- 100%[===================>]   5.11M  3.47MB/s    in 1.5s    \n","\n","2018-12-11 06:52:38 (3.47 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [5363700/5363700]\n","\n","Archive:  ngrok-stable-linux-amd64.zip\n","replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n","  inflating: ngrok                   \n","https://acec6805.ngrok.io\n"],"name":"stdout"}]},{"metadata":{"id":"DE1n9EXCpBuY","colab_type":"code","outputId":"17fd2bb7-31d9-4239-d5e8-7d968f25bd3a","executionInfo":{"status":"ok","timestamp":1544511129961,"user_tz":-540,"elapsed":20395,"user":{"displayName":"DL user","photoUrl":"","userId":"07468112820899660153"}},"colab":{"base_uri":"https://localhost:8080/","height":126}},"cell_type":"code","source":["# Simple MNIST Autoencoder in TensorFlow\n","# https://gertjanvandenburg.com/blog/autoencoder/\n","# https://github.com/GjjvdBurg/TensorFlowExperiments/tree/master/autoencoder\n","\n","# https://github.com/GjjvdBurg/TensorFlowExperiments/blob/master/autoencoder/Makefile\n","!mkdir -p ./logs\n","!mkdir -p ./images\n","!ls -l"],"execution_count":0,"outputs":[{"output_type":"stream","text":["total 20996\n","drwxr-xr-x 2 root root     4096 Dec 11 06:52 images\n","drwxr-xr-x 2 root root     4096 Dec 11 06:52 logs\n","-rwxr-xr-x 1 root root 16117632 Jul 15  2017 ngrok\n","-rw-r--r-- 1 root root  5363700 Dec 11 06:51 ngrok-stable-linux-amd64.zip\n","drwxr-xr-x 1 root root     4096 Dec  7 17:32 sample_data\n"],"name":"stdout"}]},{"metadata":{"id":"vZ-DsHXmDyRv","colab_type":"code","outputId":"d05075ff-6efa-4f82-f867-0e3645ddbe36","executionInfo":{"status":"ok","timestamp":1544512058873,"user_tz":-540,"elapsed":65679,"user":{"displayName":"DL user","photoUrl":"","userId":"07468112820899660153"}},"colab":{"base_uri":"https://localhost:8080/","height":3237}},"cell_type":"code","source":["# Environment Setup for magenta\n","# https://colab.research.google.com/notebooks/magenta/performance_rnn/performance_rnn.ipynb#scrollTo=nzGyqJja7I0O\n","!apt-get update -qq && apt-get install -qq libfluidsynth1 build-essential libasound2-dev libjack-dev\n","!pip install -U magenta pyfluidsynth pretty_midi\n","\n","# Hack to allow python to pick up the newly-installed fluidsynth lib.\n","import ctypes.util\n","def proxy_find_library(lib):\n","  if lib == 'fluidsynth':\n","    return 'libfluidsynth.so.1'\n","  else:\n","    return ctypes.util.find_library(lib)\n","\n","ctypes.util.find_library = proxy_find_library\n","\n","# Download Salamander piano SoundFont.\n","# Samples by Alexander Holm: https://archive.org/details/SalamanderGrandPianoV3\n","# Converted to sf2 by John Nebauer: https://sites.google.com/site/soundfonts4u\n","!gsutil -m cp gs://download.magenta.tensorflow.org/soundfonts/Yamaha-C5-Salamander-JNv5.1.sf2 /tmp/"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Selecting previously unselected package libogg0:amd64.\n","(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 26397 files and directories currently installed.)\n","Preparing to unpack .../00-libogg0_1.3.2-1_amd64.deb ...\n","Unpacking libogg0:amd64 (1.3.2-1) ...\n","Selecting previously unselected package libasound2-data.\n","Preparing to unpack .../01-libasound2-data_1.1.3-5ubuntu0.1_all.deb ...\n","Unpacking libasound2-data (1.1.3-5ubuntu0.1) ...\n","Selecting previously unselected package libasound2:amd64.\n","Preparing to unpack .../02-libasound2_1.1.3-5ubuntu0.1_amd64.deb ...\n","Unpacking libasound2:amd64 (1.1.3-5ubuntu0.1) ...\n","Selecting previously unselected package libasound2-dev:amd64.\n","Preparing to unpack .../03-libasound2-dev_1.1.3-5ubuntu0.1_amd64.deb ...\n","Unpacking libasound2-dev:amd64 (1.1.3-5ubuntu0.1) ...\n","Selecting previously unselected package libasyncns0:amd64.\n","Preparing to unpack .../04-libasyncns0_0.8-6_amd64.deb ...\n","Unpacking libasyncns0:amd64 (0.8-6) ...\n","Selecting previously unselected package libflac8:amd64.\n","Preparing to unpack .../05-libflac8_1.3.2-1_amd64.deb ...\n","Unpacking libflac8:amd64 (1.3.2-1) ...\n","Selecting previously unselected package libjack0:amd64.\n","Preparing to unpack .../06-libjack0_1%3a0.125.0-3_amd64.deb ...\n","Unpacking libjack0:amd64 (1:0.125.0-3) ...\n","Selecting previously unselected package libvorbis0a:amd64.\n","Preparing to unpack .../07-libvorbis0a_1.3.5-4.2_amd64.deb ...\n","Unpacking libvorbis0a:amd64 (1.3.5-4.2) ...\n","Selecting previously unselected package libvorbisenc2:amd64.\n","Preparing to unpack .../08-libvorbisenc2_1.3.5-4.2_amd64.deb ...\n","Unpacking libvorbisenc2:amd64 (1.3.5-4.2) ...\n","Selecting previously unselected package libsndfile1:amd64.\n","Preparing to unpack .../09-libsndfile1_1.0.28-4_amd64.deb ...\n","Unpacking libsndfile1:amd64 (1.0.28-4) ...\n","Selecting previously unselected package libwrap0:amd64.\n","Preparing to unpack .../10-libwrap0_7.6.q-27_amd64.deb ...\n","Unpacking libwrap0:amd64 (7.6.q-27) ...\n","Selecting previously unselected package libpulse0:amd64.\n","Preparing to unpack .../11-libpulse0_1%3a11.1-1ubuntu7.1_amd64.deb ...\n","Unpacking libpulse0:amd64 (1:11.1-1ubuntu7.1) ...\n","Selecting previously unselected package libfluidsynth1:amd64.\n","Preparing to unpack .../12-libfluidsynth1_1.1.9-1_amd64.deb ...\n","Unpacking libfluidsynth1:amd64 (1.1.9-1) ...\n","Selecting previously unselected package uuid-dev:amd64.\n","Preparing to unpack .../13-uuid-dev_2.31.1-0.4ubuntu3.2_amd64.deb ...\n","Unpacking uuid-dev:amd64 (2.31.1-0.4ubuntu3.2) ...\n","Selecting previously unselected package libjack-dev.\n","Preparing to unpack .../14-libjack-dev_1%3a0.125.0-3_amd64.deb ...\n","Unpacking libjack-dev (1:0.125.0-3) ...\n","Setting up libasyncns0:amd64 (0.8-6) ...\n","Setting up libjack0:amd64 (1:0.125.0-3) ...\n","Setting up libasound2-data (1.1.3-5ubuntu0.1) ...\n","Setting up uuid-dev:amd64 (2.31.1-0.4ubuntu3.2) ...\n","Setting up libasound2:amd64 (1.1.3-5ubuntu0.1) ...\n","Setting up libogg0:amd64 (1.3.2-1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Setting up libvorbis0a:amd64 (1.3.5-4.2) ...\n","Setting up libwrap0:amd64 (7.6.q-27) ...\n","Setting up libasound2-dev:amd64 (1.1.3-5ubuntu0.1) ...\n","Setting up libjack-dev (1:0.125.0-3) ...\n","Setting up libflac8:amd64 (1.3.2-1) ...\n","Setting up libvorbisenc2:amd64 (1.3.5-4.2) ...\n","Setting up libsndfile1:amd64 (1.0.28-4) ...\n","Setting up libpulse0:amd64 (1:11.1-1ubuntu7.1) ...\n","Setting up libfluidsynth1:amd64 (1.1.9-1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1) ...\n","Collecting magenta\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/92/d8991c8156d7a2ce50e68d02741610ed977deaf052de0fddab064eef86d9/magenta-0.4.0-py2.py3-none-any.whl (1.1MB)\n","\u001b[K    100% |████████████████████████████████| 1.1MB 8.5MB/s \n","\u001b[?25hCollecting pyfluidsynth\n","  Downloading https://files.pythonhosted.org/packages/b6/45/7f06ef269e14fb58b3a082beff9399662cc6b522692eb0be9786b23055e0/pyFluidSynth-1.2.5-py3-none-any.whl\n","Collecting pretty_midi\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/82/ee67696b85ca3be267c67a46595545e719eec677dcd94e3cf827db833fb8/pretty_midi-0.2.8.tar.gz (5.6MB)\n","\u001b[K    100% |████████████████████████████████| 5.6MB 1.2MB/s \n","\u001b[?25hCollecting intervaltree>=2.1.0 (from magenta)\n","  Downloading https://files.pythonhosted.org/packages/ca/c1/450d109b70fa58ca9d77972b02f69222412f9175ccf99fdeaf167be9583c/intervaltree-2.1.0.tar.gz\n","Requirement already satisfied, skipping upgrade: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from magenta) (1.14.6)\n","Collecting librosa>=0.6.2 (from magenta)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/b4/5b411f19de48f8fc1a0ff615555aa9124952e4156e94d4803377e50cfa4c/librosa-0.6.2.tar.gz (1.6MB)\n","\u001b[K    100% |████████████████████████████████| 1.6MB 18.0MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: pandas>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from magenta) (0.22.0)\n","Collecting python-rtmidi (from magenta)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/78/16f6d12abfccb6b1d330b1e1c6228536fa8d8ef6c14d560aba20176f044c/python-rtmidi-1.1.2.tar.gz (204kB)\n","\u001b[K    100% |████████████████████████████████| 204kB 26.6MB/s \n","\u001b[?25hCollecting bokeh>=0.12.0 (from magenta)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/4f/e6554176080d5cb809a20f36b8723ded05872c60f72e791efd6f2a9346bd/bokeh-1.0.2.tar.gz (16.2MB)\n","\u001b[K    100% |████████████████████████████████| 16.2MB 1.9MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from magenta) (1.1.0)\n","Requirement already satisfied, skipping upgrade: IPython in /usr/local/lib/python3.6/dist-packages (from magenta) (5.5.0)\n","Requirement already satisfied, skipping upgrade: matplotlib>=1.5.3 in /usr/local/lib/python3.6/dist-packages (from magenta) (2.1.2)\n","Requirement already satisfied, skipping upgrade: wheel in /usr/local/lib/python3.6/dist-packages (from magenta) (0.32.3)\n","Collecting mido==1.2.6 (from magenta)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/51/447066f537e05996a4579829b93390a4d85b0e3da90c5fbc34c1e70a37d5/mido-1.2.6-py2.py3-none-any.whl (69kB)\n","\u001b[K    100% |████████████████████████████████| 71kB 22.4MB/s \n","\u001b[?25hCollecting backports.tempfile (from magenta)\n","  Downloading https://files.pythonhosted.org/packages/b4/5c/077f910632476281428fe254807952eb47ca78e720d059a46178c541e669/backports.tempfile-1.0-py2.py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: Pillow>=3.4.2 in /usr/local/lib/python3.6/dist-packages (from magenta) (4.0.0)\n","Requirement already satisfied, skipping upgrade: tensorflow-probability>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from magenta) (0.5.0)\n","Requirement already satisfied, skipping upgrade: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from magenta) (0.13.0)\n","Requirement already satisfied, skipping upgrade: tensorflow>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from magenta) (1.12.0)\n","Collecting mir-eval>=0.4 (from magenta)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/dc/a22af4ad364742e65922fb8bf0de14d63b6ec3e08ae7ce20fad522b999b7/mir_eval-0.5.tar.gz (86kB)\n","\u001b[K    100% |████████████████████████████████| 92kB 17.7MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from pyfluidsynth) (0.16.0)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from pretty_midi) (1.11.0)\n","Collecting sortedcontainers (from intervaltree>=2.1.0->magenta)\n","  Downloading https://files.pythonhosted.org/packages/13/f3/cf85f7c3a2dbd1a515d51e1f1676d971abe41bba6f4ab5443240d9a78e5b/sortedcontainers-2.1.0-py2.py3-none-any.whl\n","Collecting audioread>=2.0.0 (from librosa>=0.6.2->magenta)\n","  Downloading https://files.pythonhosted.org/packages/f0/41/8cd160c6b2046b997d571a744a7f398f39e954a62dd747b2aae1ad7f07d4/audioread-2.1.6.tar.gz\n","Requirement already satisfied, skipping upgrade: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.6.2->magenta) (0.19.2)\n","Requirement already satisfied, skipping upgrade: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.6.2->magenta) (4.3.0)\n","Collecting resampy>=0.2.0 (from librosa>=0.6.2->magenta)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/b6/66a06d85474190b50aee1a6c09cdc95bb405ac47338b27e9b21409da1760/resampy-0.2.1.tar.gz (322kB)\n","\u001b[K    100% |████████████████████████████████| 327kB 24.0MB/s \n","\u001b[?25hCollecting numba>=0.38.0 (from librosa>=0.6.2->magenta)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/55/938f0023a4f37fe24460d46846670aba8170a6b736f1693293e710d4a6d0/numba-0.41.0-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n","\u001b[K    100% |████████████████████████████████| 3.2MB 11.1MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.1->magenta) (2.5.3)\n","Requirement already satisfied, skipping upgrade: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.1->magenta) (2018.7)\n","Requirement already satisfied, skipping upgrade: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from bokeh>=0.12.0->magenta) (3.13)\n","Requirement already satisfied, skipping upgrade: Jinja2>=2.7 in /usr/local/lib/python3.6/dist-packages (from bokeh>=0.12.0->magenta) (2.10)\n","Collecting packaging>=16.8 (from bokeh>=0.12.0->magenta)\n","  Downloading https://files.pythonhosted.org/packages/89/d1/92e6df2e503a69df9faab187c684585f0136662c12bb1f36901d426f3fab/packaging-18.0-py2.py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: tornado>=4.3 in /usr/local/lib/python3.6/dist-packages (from bokeh>=0.12.0->magenta) (4.5.3)\n","Requirement already satisfied, skipping upgrade: pickleshare in /usr/local/lib/python3.6/dist-packages (from IPython->magenta) (0.7.5)\n","Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from IPython->magenta) (4.6.0)\n","Requirement already satisfied, skipping upgrade: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from IPython->magenta) (40.6.2)\n","Requirement already satisfied, skipping upgrade: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from IPython->magenta) (0.8.1)\n","Requirement already satisfied, skipping upgrade: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from IPython->magenta) (4.3.2)\n","Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.6/dist-packages (from IPython->magenta) (2.1.3)\n","Requirement already satisfied, skipping upgrade: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from IPython->magenta) (1.0.15)\n","Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.3->magenta) (0.10.0)\n","Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.3->magenta) (2.3.0)\n","Collecting backports.weakref (from backports.tempfile->magenta)\n","  Downloading https://files.pythonhosted.org/packages/88/ec/f598b633c3d5ffe267aaada57d961c94fdfa183c5c3ebda2b6d151943db6/backports.weakref-1.0.post1-py2.py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow>=3.4.2->magenta) (0.46)\n","Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->magenta) (1.0.5)\n","Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->magenta) (1.15.0)\n","Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->magenta) (0.7.1)\n","Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->magenta) (1.0.6)\n","Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->magenta) (3.6.1)\n","Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->magenta) (1.1.0)\n","Requirement already satisfied, skipping upgrade: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->magenta) (1.12.0)\n","Requirement already satisfied, skipping upgrade: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->magenta) (0.6.1)\n","Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->magenta) (0.2.0)\n","Collecting llvmlite>=0.26.0dev0 (from numba>=0.38.0->librosa>=0.6.2->magenta)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/60/d22966c97a47687ac1cc57c2e756380897c264f1ce40780105d7dbcd9564/llvmlite-0.26.0-cp36-cp36m-manylinux1_x86_64.whl (16.1MB)\n","\u001b[K    100% |████████████████████████████████| 16.1MB 3.3MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7->bokeh>=0.12.0->magenta) (1.1.0)\n","Requirement already satisfied, skipping upgrade: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->IPython->magenta) (0.6.0)\n","Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->IPython->magenta) (0.2.0)\n","Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->magenta) (0.1.7)\n","Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow>=1.12.0->magenta) (2.8.0)\n","Requirement already satisfied, skipping upgrade: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow>=1.12.0->magenta) (0.14.1)\n","Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow>=1.12.0->magenta) (3.0.1)\n","Building wheels for collected packages: pretty-midi, intervaltree, librosa, python-rtmidi, bokeh, mir-eval, audioread, resampy\n","  Running setup.py bdist_wheel for pretty-midi ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/22/e7/6b/70eb5879f7dbcb4f44fee735a61d6298f9e082be8538b52422\n","  Running setup.py bdist_wheel for intervaltree ... \u001b[?25l-\b \b\\\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/6b/cf/b0/f7ef2d0f504d26f3e9e70c2369e5725591ccfaf67d528fcbc5\n","  Running setup.py bdist_wheel for librosa ... \u001b[?25l-\b \b\\\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/18/b8/10/f0f8f6ac60668a5cd75596cf14c25bb6b3ea1ecd815f058b7e\n","  Running setup.py bdist_wheel for python-rtmidi ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/b4/9d/55/0554f02fbc777976865e0ea603028a0f87854b7df385ffa366\n","  Running setup.py bdist_wheel for bokeh ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/1a/a0/ec/d46994ac427b4879969dd780cf422bd3a0886fb85f481dd064\n","  Running setup.py bdist_wheel for mir-eval ... \u001b[?25l-\b \b\\\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/69/d2/fe/892fae0039b51e3774a92daac135e45268ff5f52f28b99f4e4\n","  Running setup.py bdist_wheel for audioread ... \u001b[?25l-\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/53/02/90/7b5c4081b7470c550ab605f600bad237dde12a6b8999b11f50\n","  Running setup.py bdist_wheel for resampy ... \u001b[?25l-\b \b\\\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ff/4f/ed/2e6c676c23efe5394bb40ade50662e90eb46e29b48324c5f9b\n","Successfully built pretty-midi intervaltree librosa python-rtmidi bokeh mir-eval audioread resampy\n","Installing collected packages: sortedcontainers, intervaltree, audioread, llvmlite, numba, resampy, librosa, mido, pretty-midi, python-rtmidi, packaging, bokeh, backports.weakref, backports.tempfile, mir-eval, magenta, pyfluidsynth\n","Successfully installed audioread-2.1.6 backports.tempfile-1.0 backports.weakref-1.0.post1 bokeh-1.0.2 intervaltree-2.1.0 librosa-0.6.2 llvmlite-0.26.0 magenta-0.4.0 mido-1.2.6 mir-eval-0.5 numba-0.41.0 packaging-18.0 pretty-midi-0.2.8 pyfluidsynth-1.2.5 python-rtmidi-1.1.2 resampy-0.2.1 sortedcontainers-2.1.0\n","Copying gs://download.magenta.tensorflow.org/soundfonts/Yamaha-C5-Salamander-JNv5.1.sf2...\n","| [1/1 files][591.9 MiB/591.9 MiB] 100% Done  39.1 MiB/s ETA 00:00:00           \n","Operation completed over 1 objects/591.9 MiB.                                    \n"],"name":"stdout"}]},{"metadata":{"id":"sBTqgAjE7zGy","colab_type":"code","outputId":"fdd08172-4681-4f62-eeba-1024fadd9d45","executionInfo":{"status":"ok","timestamp":1544512652291,"user_tz":-540,"elapsed":561686,"user":{"displayName":"DL user","photoUrl":"","userId":"07468112820899660153"}},"colab":{"base_uri":"https://localhost:8080/","height":10092}},"cell_type":"code","source":["#!/usr/bin/env python\n","\n","\"\"\"TensorFlow MNIST AutoEncoder\n","\n","This is my attempt to write the autoencoder for MNIST by Andrej Karpathy using \n","ConvNetJS in TensorFlow. Mostly to get some more experience working in \n","Tensorflow.\n","\n","Sources:\n","    - http://cs.stanford.edu/people/karpathy/convnetjs/demo/autoencoder.html\n","    - https://www.tensorflow.org/get_started/mnist/pros\n","\n","Author: Gertjan van den Burg\n","Date: Thu Oct 26 16:49:29 CEST 2017\n","\n","\"\"\"\n","\n","import numpy as np\n","import tensorflow as tf\n","\n","from magenta.models.image_stylization.image_utils import form_image_grid\n","from tensorflow.examples.tutorials.mnist import input_data\n","\n","BATCH_SIZE = 50\n","GRID_ROWS = 5\n","GRID_COLS = 10\n","USE_RELU = False\n","\n","\n","def weight_variable(shape):\n","    # From the mnist tutorial\n","    initial = tf.truncated_normal(shape, stddev=0.1)\n","    return tf.Variable(initial)\n","\n","\n","def bias_variable(shape):\n","    initial = tf.constant(0.1, shape=shape)\n","    return tf.Variable(initial)\n","\n","\n","def fc_layer(previous, input_size, output_size):\n","    W = weight_variable([input_size, output_size])\n","    b = bias_variable([output_size])\n","    return tf.matmul(previous, W) + b\n","\n","\n","def autoencoder(x):\n","    # first fully connected layer with 50 neurons using tanh activation\n","    l1 = tf.nn.tanh(fc_layer(x, 28*28, 50))\n","    # second fully connected layer with 50 neurons using tanh activation\n","    l2 = tf.nn.tanh(fc_layer(l1, 50, 50))\n","    # third fully connected layer with 2 neurons\n","    l3 = fc_layer(l2, 50, 2)\n","    # fourth fully connected layer with 50 neurons and tanh activation\n","    l4 = tf.nn.tanh(fc_layer(l3, 2, 50))\n","    # fifth fully connected layer with 50 neurons and tanh activation\n","    l5 = tf.nn.tanh(fc_layer(l4, 50, 50))\n","    # readout layer\n","    if USE_RELU:\n","        out = tf.nn.relu(fc_layer(l5, 50, 28*28))\n","    else:\n","        out = fc_layer(l5, 50, 28*28)\n","    # let's use an l2 loss on the output image\n","    loss = tf.reduce_mean(tf.squared_difference(x, out))\n","    return loss, out, l3\n","\n","\n","def layer_grid_summary(name, var, image_dims):\n","    prod = np.prod(image_dims)\n","    grid = form_image_grid(tf.reshape(var, [BATCH_SIZE, prod]), [GRID_ROWS, \n","        GRID_COLS], image_dims, 1)\n","    return tf.summary.image(name, grid)\n","\n","\n","def create_summaries(loss, x, latent, output):\n","    writer = tf.summary.FileWriter(\"./logs\")\n","    tf.summary.scalar(\"Loss\", loss)\n","    layer_grid_summary(\"Input\", x, [28, 28])\n","    layer_grid_summary(\"Encoder\", latent, [2, 1])\n","    layer_grid_summary(\"Output\", output, [28, 28])\n","    return writer, tf.summary.merge_all()\n","\n","\n","def make_image(name, var, image_dims):\n","    prod = np.prod(image_dims)\n","    grid = form_image_grid(tf.reshape(var, [BATCH_SIZE, prod]), [GRID_ROWS, \n","        GRID_COLS], image_dims, 1)\n","    s_grid = tf.squeeze(grid, axis=0)\n","\n","    # This reproduces the code in: tensorflow/core/kernels/summary_image_op.cc\n","    im_min = tf.reduce_min(s_grid)\n","    im_max = tf.reduce_max(s_grid)\n","\n","    kZeroThreshold = tf.constant(1e-6)\n","    max_val = tf.maximum(tf.abs(im_min), tf.abs(im_max))\n","\n","    offset = tf.cond(\n","            im_min < tf.constant(0.0),\n","            lambda: tf.constant(128.0),\n","            lambda: tf.constant(0.0)\n","            )\n","    scale = tf.cond(\n","            im_min < tf.constant(0.0),\n","            lambda: tf.cond(\n","                max_val < kZeroThreshold,\n","                lambda: tf.constant(0.0),\n","                lambda: tf.div(127.0, max_val)\n","                ),\n","            lambda: tf.cond(\n","                im_max < kZeroThreshold,\n","                lambda: tf.constant(0.0),\n","                lambda: tf.div(255.0, im_max)\n","                )\n","            )\n","    s_grid = tf.cast(tf.add(tf.multiply(s_grid, scale), offset), tf.uint8)\n","    enc = tf.image.encode_jpeg(s_grid)\n","\n","    fwrite = tf.write_file(name, enc)\n","    return fwrite\n","\n","\n","def main():\n","    # initialize the data\n","    mnist = input_data.read_data_sets('/tmp/MNIST_data')\n","\n","    # placeholders for the images\n","    x = tf.placeholder(tf.float32, shape=[None, 784])\n","\n","    # build the model\n","    loss, output, latent = autoencoder(x)\n","\n","    # and we use the Adam Optimizer for training\n","    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n","\n","    # We want to use Tensorboard to visualize some stuff\n","    writer, summary_op = create_summaries(loss, x, latent, output)\n","\n","    first_batch = mnist.train.next_batch(BATCH_SIZE)\n","\n","    # Run the training loop\n","    with tf.Session() as sess:\n","        sess.run(tf.global_variables_initializer())\n","        sess.run(make_image(\"images/input.jpg\", x, [28, 28]), feed_dict={x : \n","            first_batch[0]})\n","        for i in range(int(200001)):\n","            batch = mnist.train.next_batch(BATCH_SIZE)\n","            feed = {x : batch[0]}\n","            if i % 500 == 0:\n","                summary, train_loss = sess.run([summary_op, loss], \n","                        feed_dict=feed)\n","                print(\"step %d, training loss: %g\" % (i, train_loss))\n","\n","                writer.add_summary(summary, i)\n","                writer.flush()\n","\n","            if i % 1000 == 0:\n","                sess.run(make_image(\"images/output_%06i.jpg\" % i, output, [28, \n","                    28]), feed_dict={x : first_batch[0]})\n","\n","            train_step.run(feed_dict=feed)\n","\n","        # Save latent space\n","        pred = sess.run(latent, feed_dict={x : mnist.test._images})\n","        pred = np.asarray(pred)\n","        pred = np.reshape(pred, (mnist.test._num_examples, 2))\n","        labels = np.reshape(mnist.test._labels, (mnist.test._num_examples, 1))\n","        pred = np.hstack((pred, labels))\n","        if USE_RELU:\n","            fname = \"latent_relu.csv\"\n","        else:\n","            fname = \"latent_default.csv\"\n","        np.savetxt(fname, pred)\n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n","/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n","  return f(*args, **kwds)\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-9-32a7c706c3a8>:123: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please write your own downloading logic.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use urllib or similar directly.\n","Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.data to implement this functionality.\n","Extracting /tmp/MNIST_data/train-images-idx3-ubyte.gz\n","Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.data to implement this functionality.\n","Extracting /tmp/MNIST_data/train-labels-idx1-ubyte.gz\n","Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n","Extracting /tmp/MNIST_data/t10k-images-idx3-ubyte.gz\n","Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n","Extracting /tmp/MNIST_data/t10k-labels-idx1-ubyte.gz\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n","step 0, training loss: 0.104465\n","step 500, training loss: 0.0670278\n","step 1000, training loss: 0.0585517\n","step 1500, training loss: 0.0590774\n","step 2000, training loss: 0.0568431\n","step 2500, training loss: 0.0594738\n","step 3000, training loss: 0.0619073\n","step 3500, training loss: 0.0545794\n","step 4000, training loss: 0.058403\n","step 4500, training loss: 0.0525341\n","step 5000, training loss: 0.0561085\n","step 5500, training loss: 0.0551406\n","step 6000, training loss: 0.0516821\n","step 6500, training loss: 0.0560227\n","step 7000, training loss: 0.0535967\n","step 7500, training loss: 0.0519543\n","step 8000, training loss: 0.0584517\n","step 8500, training loss: 0.0485356\n","step 9000, training loss: 0.0538583\n","step 9500, training loss: 0.0507451\n","step 10000, training loss: 0.0509339\n","step 10500, training loss: 0.0487002\n","step 11000, training loss: 0.0536499\n","step 11500, training loss: 0.0471274\n","step 12000, training loss: 0.0529276\n","step 12500, training loss: 0.0483257\n","step 13000, training loss: 0.0470526\n","step 13500, training loss: 0.0472125\n","step 14000, training loss: 0.0543169\n","step 14500, training loss: 0.0485403\n","step 15000, training loss: 0.0515801\n","step 15500, training loss: 0.0537167\n","step 16000, training loss: 0.0515917\n","step 16500, training loss: 0.0485996\n","step 17000, training loss: 0.0513009\n","step 17500, training loss: 0.0457575\n","step 18000, training loss: 0.0434549\n","step 18500, training loss: 0.0495548\n","step 19000, training loss: 0.0476836\n","step 19500, training loss: 0.0486866\n","step 20000, training loss: 0.0477073\n","step 20500, training loss: 0.0436161\n","step 21000, training loss: 0.0450775\n","step 21500, training loss: 0.0464776\n","step 22000, training loss: 0.0438327\n","step 22500, training loss: 0.0430371\n","step 23000, training loss: 0.047425\n","step 23500, training loss: 0.0501238\n","step 24000, training loss: 0.047339\n","step 24500, training loss: 0.0407443\n","step 25000, training loss: 0.0474639\n","step 25500, training loss: 0.0501103\n","step 26000, training loss: 0.0485015\n","step 26500, training loss: 0.0474384\n","step 27000, training loss: 0.0461981\n","step 27500, training loss: 0.0455411\n","step 28000, training loss: 0.046726\n","step 28500, training loss: 0.0444888\n","step 29000, training loss: 0.0461419\n","step 29500, training loss: 0.0461586\n","step 30000, training loss: 0.047252\n","step 30500, training loss: 0.0419391\n","step 31000, training loss: 0.0453229\n","step 31500, training loss: 0.0439437\n","step 32000, training loss: 0.0489876\n","step 32500, training loss: 0.0440482\n","step 33000, training loss: 0.0474139\n","step 33500, training loss: 0.0407386\n","step 34000, training loss: 0.0449517\n","step 34500, training loss: 0.0435107\n","step 35000, training loss: 0.0441949\n","step 35500, training loss: 0.0481203\n","step 36000, training loss: 0.0434835\n","step 36500, training loss: 0.0499786\n","step 37000, training loss: 0.0389691\n","step 37500, training loss: 0.0413555\n","step 38000, training loss: 0.0440957\n","step 38500, training loss: 0.0443159\n","step 39000, training loss: 0.045138\n","step 39500, training loss: 0.0408434\n","step 40000, training loss: 0.0428481\n","step 40500, training loss: 0.0440443\n","step 41000, training loss: 0.0445029\n","step 41500, training loss: 0.0444377\n","step 42000, training loss: 0.0411592\n","step 42500, training loss: 0.0447331\n","step 43000, training loss: 0.0481712\n","step 43500, training loss: 0.0433695\n","step 44000, training loss: 0.0444701\n","step 44500, training loss: 0.0461517\n","step 45000, training loss: 0.0446456\n","step 45500, training loss: 0.0394189\n","step 46000, training loss: 0.0421144\n","step 46500, training loss: 0.0468529\n","step 47000, training loss: 0.043767\n","step 47500, training loss: 0.0464913\n","step 48000, training loss: 0.0447205\n","step 48500, training loss: 0.040323\n","step 49000, training loss: 0.0391791\n","step 49500, training loss: 0.0407877\n","step 50000, training loss: 0.0428119\n","step 50500, training loss: 0.0422461\n","step 51000, training loss: 0.042685\n","step 51500, training loss: 0.0408911\n","step 52000, training loss: 0.0436689\n","step 52500, training loss: 0.0402814\n","step 53000, training loss: 0.0440449\n","step 53500, training loss: 0.0463476\n","step 54000, training loss: 0.0407363\n","step 54500, training loss: 0.0395674\n","step 55000, training loss: 0.0435887\n","step 55500, training loss: 0.048986\n","step 56000, training loss: 0.0364625\n","step 56500, training loss: 0.0450133\n","step 57000, training loss: 0.0423686\n","step 57500, training loss: 0.0415518\n","step 58000, training loss: 0.0414512\n","step 58500, training loss: 0.0436186\n","step 59000, training loss: 0.0461412\n","step 59500, training loss: 0.0379987\n","step 60000, training loss: 0.0444737\n","step 60500, training loss: 0.0426135\n","step 61000, training loss: 0.0419281\n","step 61500, training loss: 0.040997\n","step 62000, training loss: 0.041272\n","step 62500, training loss: 0.0426141\n","step 63000, training loss: 0.0457239\n","step 63500, training loss: 0.0460554\n","step 64000, training loss: 0.042658\n","step 64500, training loss: 0.0425479\n","step 65000, training loss: 0.0414519\n","step 65500, training loss: 0.040675\n","step 66000, training loss: 0.0424741\n","step 66500, training loss: 0.0418649\n","step 67000, training loss: 0.0423639\n","step 67500, training loss: 0.0391991\n","step 68000, training loss: 0.0388717\n","step 68500, training loss: 0.040855\n","step 69000, training loss: 0.0375024\n","step 69500, training loss: 0.0410547\n","step 70000, training loss: 0.0406823\n","step 70500, training loss: 0.0439837\n","step 71000, training loss: 0.0414336\n","step 71500, training loss: 0.0369237\n","step 72000, training loss: 0.0377754\n","step 72500, training loss: 0.0394352\n","step 73000, training loss: 0.038284\n","step 73500, training loss: 0.0451312\n","step 74000, training loss: 0.0371468\n","step 74500, training loss: 0.0402104\n","step 75000, training loss: 0.0410663\n","step 75500, training loss: 0.0370937\n","step 76000, training loss: 0.0440195\n","step 76500, training loss: 0.0392514\n","step 77000, training loss: 0.037684\n","step 77500, training loss: 0.0387302\n","step 78000, training loss: 0.0397889\n","step 78500, training loss: 0.0433189\n","step 79000, training loss: 0.0423688\n","step 79500, training loss: 0.0367438\n","step 80000, training loss: 0.0412029\n","step 80500, training loss: 0.0407196\n","step 81000, training loss: 0.0471451\n","step 81500, training loss: 0.0407825\n","step 82000, training loss: 0.0409549\n","step 82500, training loss: 0.0416903\n","step 83000, training loss: 0.0387277\n","step 83500, training loss: 0.03745\n","step 84000, training loss: 0.0421354\n","step 84500, training loss: 0.0406968\n","step 85000, training loss: 0.0410534\n","step 85500, training loss: 0.0358356\n","step 86000, training loss: 0.0415748\n","step 86500, training loss: 0.0421103\n","step 87000, training loss: 0.0387005\n","step 87500, training loss: 0.0390411\n","step 88000, training loss: 0.0356572\n","step 88500, training loss: 0.0421995\n","step 89000, training loss: 0.0381307\n","step 89500, training loss: 0.0426018\n","step 90000, training loss: 0.0392734\n","step 90500, training loss: 0.0383192\n","step 91000, training loss: 0.0411102\n","step 91500, training loss: 0.0432175\n","step 92000, training loss: 0.0398769\n","step 92500, training loss: 0.0444406\n","step 93000, training loss: 0.0416453\n","step 93500, training loss: 0.0406641\n","step 94000, training loss: 0.0430801\n","step 94500, training loss: 0.0419239\n","step 95000, training loss: 0.0440681\n","step 95500, training loss: 0.033718\n","step 96000, training loss: 0.0390354\n","step 96500, training loss: 0.0382959\n","step 97000, training loss: 0.0385552\n","step 97500, training loss: 0.0402239\n","step 98000, training loss: 0.0394454\n","step 98500, training loss: 0.0384641\n","step 99000, training loss: 0.0361897\n","step 99500, training loss: 0.0392324\n","step 100000, training loss: 0.0388273\n","step 100500, training loss: 0.0377979\n","step 101000, training loss: 0.0417822\n","step 101500, training loss: 0.038439\n","step 102000, training loss: 0.0393477\n","step 102500, training loss: 0.0403159\n","step 103000, training loss: 0.0363473\n","step 103500, training loss: 0.0382538\n","step 104000, training loss: 0.0397556\n","step 104500, training loss: 0.0364779\n","step 105000, training loss: 0.0426803\n","step 105500, training loss: 0.0364323\n","step 106000, training loss: 0.040461\n","step 106500, training loss: 0.0376761\n","step 107000, training loss: 0.0360527\n","step 107500, training loss: 0.04226\n","step 108000, training loss: 0.0388963\n","step 108500, training loss: 0.0388565\n","step 109000, training loss: 0.0397145\n","step 109500, training loss: 0.0366496\n","step 110000, training loss: 0.0379937\n","step 110500, training loss: 0.0373754\n","step 111000, training loss: 0.0349986\n","step 111500, training loss: 0.0401827\n","step 112000, training loss: 0.0374978\n","step 112500, training loss: 0.0422986\n","step 113000, training loss: 0.0404864\n","step 113500, training loss: 0.0355647\n","step 114000, training loss: 0.0381204\n","step 114500, training loss: 0.0377104\n","step 115000, training loss: 0.0356969\n","step 115500, training loss: 0.0415198\n","step 116000, training loss: 0.0403813\n","step 116500, training loss: 0.04128\n","step 117000, training loss: 0.0403496\n","step 117500, training loss: 0.0373169\n","step 118000, training loss: 0.0376623\n","step 118500, training loss: 0.038186\n","step 119000, training loss: 0.0394936\n","step 119500, training loss: 0.0392964\n","step 120000, training loss: 0.0447324\n","step 120500, training loss: 0.0363067\n","step 121000, training loss: 0.0371359\n","step 121500, training loss: 0.0411771\n","step 122000, training loss: 0.038194\n","step 122500, training loss: 0.0385521\n","step 123000, training loss: 0.0425886\n","step 123500, training loss: 0.0393606\n","step 124000, training loss: 0.040667\n","step 124500, training loss: 0.0377785\n","step 125000, training loss: 0.0419873\n","step 125500, training loss: 0.0415563\n","step 126000, training loss: 0.0409074\n","step 126500, training loss: 0.0409478\n","step 127000, training loss: 0.0403513\n","step 127500, training loss: 0.0369604\n","step 128000, training loss: 0.041461\n","step 128500, training loss: 0.0410154\n","step 129000, training loss: 0.0383446\n","step 129500, training loss: 0.0403567\n","step 130000, training loss: 0.041837\n","step 130500, training loss: 0.0379473\n","step 131000, training loss: 0.0428171\n","step 131500, training loss: 0.0393796\n","step 132000, training loss: 0.0406861\n","step 132500, training loss: 0.0338918\n","step 133000, training loss: 0.0362102\n","step 133500, training loss: 0.0372196\n","step 134000, training loss: 0.0376214\n","step 134500, training loss: 0.0389134\n","step 135000, training loss: 0.0420186\n","step 135500, training loss: 0.0418083\n","step 136000, training loss: 0.0418203\n","step 136500, training loss: 0.0396733\n","step 137000, training loss: 0.0413249\n","step 137500, training loss: 0.0373721\n","step 138000, training loss: 0.0372364\n","step 138500, training loss: 0.0322313\n","step 139000, training loss: 0.0409569\n","step 139500, training loss: 0.0393684\n","step 140000, training loss: 0.0430353\n","step 140500, training loss: 0.0426935\n","step 141000, training loss: 0.0344381\n","step 141500, training loss: 0.0409211\n","step 142000, training loss: 0.0389591\n","step 142500, training loss: 0.0394294\n","step 143000, training loss: 0.0383427\n","step 143500, training loss: 0.0366543\n","step 144000, training loss: 0.0410071\n","step 144500, training loss: 0.042098\n","step 145000, training loss: 0.0405845\n","step 145500, training loss: 0.0374414\n","step 146000, training loss: 0.0364955\n","step 146500, training loss: 0.0440231\n","step 147000, training loss: 0.0358199\n","step 147500, training loss: 0.0402706\n","step 148000, training loss: 0.0412038\n","step 148500, training loss: 0.0367341\n","step 149000, training loss: 0.0400235\n","step 149500, training loss: 0.037987\n","step 150000, training loss: 0.039091\n","step 150500, training loss: 0.0397165\n","step 151000, training loss: 0.03523\n","step 151500, training loss: 0.0420373\n","step 152000, training loss: 0.0390231\n","step 152500, training loss: 0.0417119\n","step 153000, training loss: 0.0355444\n","step 153500, training loss: 0.0379599\n","step 154000, training loss: 0.0417476\n","step 154500, training loss: 0.0350188\n","step 155000, training loss: 0.0371721\n","step 155500, training loss: 0.038738\n","step 156000, training loss: 0.0396564\n","step 156500, training loss: 0.033719\n","step 157000, training loss: 0.0392711\n","step 157500, training loss: 0.0400781\n","step 158000, training loss: 0.0365373\n","step 158500, training loss: 0.0382293\n","step 159000, training loss: 0.0389635\n","step 159500, training loss: 0.0372267\n","step 160000, training loss: 0.0372381\n","step 160500, training loss: 0.0347492\n","step 161000, training loss: 0.0391253\n","step 161500, training loss: 0.0388216\n","step 162000, training loss: 0.0371245\n","step 162500, training loss: 0.0365599\n","step 163000, training loss: 0.0360803\n","step 163500, training loss: 0.0381988\n","step 164000, training loss: 0.038316\n","step 164500, training loss: 0.0375027\n","step 165000, training loss: 0.0364113\n","step 165500, training loss: 0.0356581\n","step 166000, training loss: 0.0322284\n","step 166500, training loss: 0.0415193\n","step 167000, training loss: 0.0375029\n","step 167500, training loss: 0.0388726\n","step 168000, training loss: 0.0370367\n","step 168500, training loss: 0.0421671\n","step 169000, training loss: 0.0350481\n","step 169500, training loss: 0.0384209\n","step 170000, training loss: 0.0354402\n","step 170500, training loss: 0.0385528\n","step 171000, training loss: 0.0364866\n","step 171500, training loss: 0.0371227\n","step 172000, training loss: 0.0382351\n","step 172500, training loss: 0.0400388\n","step 173000, training loss: 0.0377303\n","step 173500, training loss: 0.0367699\n","step 174000, training loss: 0.0387749\n","step 174500, training loss: 0.0401186\n","step 175000, training loss: 0.0412341\n","step 175500, training loss: 0.0364689\n","step 176000, training loss: 0.0358532\n","step 176500, training loss: 0.0375875\n","step 177000, training loss: 0.0372787\n","step 177500, training loss: 0.0348089\n","step 178000, training loss: 0.0374212\n","step 178500, training loss: 0.0409844\n","step 179000, training loss: 0.0385456\n","step 179500, training loss: 0.0361544\n","step 180000, training loss: 0.0413093\n","step 180500, training loss: 0.0351289\n","step 181000, training loss: 0.0342131\n","step 181500, training loss: 0.0394581\n","step 182000, training loss: 0.0390489\n","step 182500, training loss: 0.0404154\n","step 183000, training loss: 0.0389333\n","step 183500, training loss: 0.0352207\n","step 184000, training loss: 0.0375955\n","step 184500, training loss: 0.0368979\n","step 185000, training loss: 0.0410717\n","step 185500, training loss: 0.0371944\n","step 186000, training loss: 0.0362806\n","step 186500, training loss: 0.0365004\n","step 187000, training loss: 0.0430024\n","step 187500, training loss: 0.0365156\n","step 188000, training loss: 0.0362327\n","step 188500, training loss: 0.0382321\n","step 189000, training loss: 0.0341751\n","step 189500, training loss: 0.0402513\n","step 190000, training loss: 0.0394099\n","step 190500, training loss: 0.037926\n","step 191000, training loss: 0.0386144\n","step 191500, training loss: 0.0433069\n","step 192000, training loss: 0.0410603\n","step 192500, training loss: 0.0367355\n","step 193000, training loss: 0.035144\n","step 193500, training loss: 0.0418196\n","step 194000, training loss: 0.0358411\n","step 194500, training loss: 0.0403387\n","step 195000, training loss: 0.0390072\n","step 195500, training loss: 0.0338372\n","step 196000, training loss: 0.0407933\n","step 196500, training loss: 0.0365574\n","step 197000, training loss: 0.0370304\n","step 197500, training loss: 0.0400872\n","step 198000, training loss: 0.0386471\n","step 198500, training loss: 0.0395247\n","step 199000, training loss: 0.0402763\n","step 199500, training loss: 0.0389802\n","step 200000, training loss: 0.0338216\n"],"name":"stdout"}]}]}