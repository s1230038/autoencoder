{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoencoderInTensorFlow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s1230038/autoencoder/blob/master/AutoencoderInTensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "VKIlmjn6LyB8",
        "colab_type": "code",
        "outputId": "2b4c633e-f09e-4feb-a1b5-d2299292a6f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -l\n",
        "#!rm -r *\n",
        "#!ls -l"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4\n",
            "drwxr-xr-x 1 root root 4096 Dec 10 17:34 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ynz-vlK_Lrpk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "# !zip -r autoencoder.zip images latent_default.csv logs\n",
        "# files.download('autoencoder.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yoG5F3YMpWSf",
        "colab_type": "code",
        "outputId": "fda56f23-d885-4a87-97ed-cf820b7117f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "# Setup ngrok and run TensorBoard on Colab\n",
        "# https://www.dlology.com/blog/quick-guide-to-run-tensorboard-in-google-colab/\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "LOG_DIR = './logs'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-12-14 07:45:01--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.165.51.142, 52.44.92.122, 54.174.228.92, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.165.51.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5363700 (5.1M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]   5.11M  21.8MB/s    in 0.2s    \n",
            "\n",
            "2018-12-14 07:45:02 (21.8 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [5363700/5363700]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n",
            "http://bc7f2033.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DE1n9EXCpBuY",
        "colab_type": "code",
        "outputId": "a1efaccc-faea-4a59-d9e0-811b5ebbb6dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "cell_type": "code",
      "source": [
        "# Simple MNIST Autoencoder in TensorFlow\n",
        "# https://gertjanvandenburg.com/blog/autoencoder/\n",
        "# https://github.com/GjjvdBurg/TensorFlowExperiments/tree/master/autoencoder\n",
        "\n",
        "# https://github.com/GjjvdBurg/TensorFlowExperiments/blob/master/autoencoder/Makefile\n",
        "!mkdir -p ./logs\n",
        "!mkdir -p ./images\n",
        "!ls -l"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 20996\n",
            "drwxr-xr-x 2 root root     4096 Dec 14 07:45 images\n",
            "drwxr-xr-x 2 root root     4096 Dec 14 07:45 logs\n",
            "-rwxr-xr-x 1 root root 16117632 Jul 15  2017 ngrok\n",
            "-rw-r--r-- 1 root root  5363700 Dec 14 07:45 ngrok-stable-linux-amd64.zip\n",
            "drwxr-xr-x 1 root root     4096 Dec 10 17:34 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vZ-DsHXmDyRv",
        "colab_type": "code",
        "outputId": "a03867e2-8a22-4b4a-aa6a-638189c25dd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1492
        }
      },
      "cell_type": "code",
      "source": [
        "# Environment Setup for magenta\n",
        "# https://colab.research.google.com/notebooks/magenta/performance_rnn/performance_rnn.ipynb#scrollTo=nzGyqJja7I0O\n",
        "!apt-get update -qq && apt-get install -qq libfluidsynth1 build-essential libasound2-dev libjack-dev\n",
        "!pip install -U magenta pyfluidsynth pretty_midi\n",
        "\n",
        "# Hack to allow python to pick up the newly-installed fluidsynth lib.\n",
        "import ctypes.util\n",
        "def proxy_find_library(lib):\n",
        "  if lib == 'fluidsynth':\n",
        "    return 'libfluidsynth.so.1'\n",
        "  else:\n",
        "    return ctypes.util.find_library(lib)\n",
        "\n",
        "ctypes.util.find_library = proxy_find_library\n",
        "\n",
        "# Download Salamander piano SoundFont.\n",
        "# Samples by Alexander Holm: https://archive.org/details/SalamanderGrandPianoV3\n",
        "# Converted to sf2 by John Nebauer: https://sites.google.com/site/soundfonts4u\n",
        "!gsutil -m cp gs://download.magenta.tensorflow.org/soundfonts/Yamaha-C5-Salamander-JNv5.1.sf2 /tmp/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package libfluidsynth1:amd64.\n",
            "(Reading database ... 110377 files and directories currently installed.)\n",
            "Preparing to unpack .../libfluidsynth1_1.1.9-1_amd64.deb ...\n",
            "Unpacking libfluidsynth1:amd64 (1.1.9-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Setting up libfluidsynth1:amd64 (1.1.9-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Collecting magenta\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/92/d8991c8156d7a2ce50e68d02741610ed977deaf052de0fddab064eef86d9/magenta-0.4.0-py2.py3-none-any.whl (1.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.1MB 11.0MB/s \n",
            "\u001b[?25hCollecting pyfluidsynth\n",
            "  Downloading https://files.pythonhosted.org/packages/b6/45/7f06ef269e14fb58b3a082beff9399662cc6b522692eb0be9786b23055e0/pyFluidSynth-1.2.5-py3-none-any.whl\n",
            "Requirement already up-to-date: pretty_midi in /usr/local/lib/python3.6/dist-packages (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib>=1.5.3 in /usr/local/lib/python3.6/dist-packages (from magenta) (2.1.2)\n",
            "Requirement already satisfied, skipping upgrade: backports.tempfile in /usr/local/lib/python3.6/dist-packages (from magenta) (1.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-probability>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from magenta) (0.5.0)\n",
            "Requirement already satisfied, skipping upgrade: python-rtmidi in /usr/local/lib/python3.6/dist-packages (from magenta) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: wheel in /usr/local/lib/python3.6/dist-packages (from magenta) (0.32.3)\n",
            "Requirement already satisfied, skipping upgrade: Pillow>=3.4.2 in /usr/local/lib/python3.6/dist-packages (from magenta) (4.0.0)\n",
            "Requirement already satisfied, skipping upgrade: mido==1.2.6 in /usr/local/lib/python3.6/dist-packages (from magenta) (1.2.6)\n",
            "Requirement already satisfied, skipping upgrade: bokeh>=0.12.0 in /usr/local/lib/python3.6/dist-packages (from magenta) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: pandas>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from magenta) (0.22.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from magenta) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: librosa>=0.6.2 in /usr/local/lib/python3.6/dist-packages (from magenta) (0.6.2)\n",
            "Requirement already satisfied, skipping upgrade: IPython in /usr/local/lib/python3.6/dist-packages (from magenta) (5.5.0)\n",
            "Requirement already satisfied, skipping upgrade: mir-eval>=0.4 in /usr/local/lib/python3.6/dist-packages (from magenta) (0.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from magenta) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from magenta) (0.13.0)\n",
            "Requirement already satisfied, skipping upgrade: intervaltree>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from magenta) (2.1.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from magenta) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from pyfluidsynth) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from pretty_midi) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.3->magenta) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.3->magenta) (2018.7)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.3->magenta) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.3->magenta) (2.5.3)\n",
            "Requirement already satisfied, skipping upgrade: backports.weakref in /usr/local/lib/python3.6/dist-packages (from backports.tempfile->magenta) (1.0.post1)\n",
            "Requirement already satisfied, skipping upgrade: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow>=3.4.2->magenta) (0.46)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from bokeh>=0.12.0->magenta) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: Jinja2>=2.7 in /usr/local/lib/python3.6/dist-packages (from bokeh>=0.12.0->magenta) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: packaging>=16.8 in /usr/local/lib/python3.6/dist-packages (from bokeh>=0.12.0->magenta) (18.0)\n",
            "Requirement already satisfied, skipping upgrade: tornado>=4.3 in /usr/local/lib/python3.6/dist-packages (from bokeh>=0.12.0->magenta) (4.5.3)\n",
            "Requirement already satisfied, skipping upgrade: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.6.2->magenta) (2.1.6)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.6.2->magenta) (0.20.1)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.6.2->magenta) (4.3.0)\n",
            "Requirement already satisfied, skipping upgrade: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.6.2->magenta) (0.2.1)\n",
            "Requirement already satisfied, skipping upgrade: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.6.2->magenta) (0.40.1)\n",
            "Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.6/dist-packages (from IPython->magenta) (2.1.3)\n",
            "Requirement already satisfied, skipping upgrade: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from IPython->magenta) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from IPython->magenta) (40.6.2)\n",
            "Requirement already satisfied, skipping upgrade: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from IPython->magenta) (4.3.2)\n",
            "Requirement already satisfied, skipping upgrade: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from IPython->magenta) (1.0.15)\n",
            "Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from IPython->magenta) (4.6.0)\n",
            "Requirement already satisfied, skipping upgrade: pickleshare in /usr/local/lib/python3.6/dist-packages (from IPython->magenta) (0.7.5)\n",
            "Requirement already satisfied, skipping upgrade: sortedcontainers in /usr/local/lib/python3.6/dist-packages (from intervaltree>=2.1.0->magenta) (2.1.0)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->magenta) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->magenta) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->magenta) (1.0.6)\n",
            "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->magenta) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->magenta) (0.6.1)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->magenta) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->magenta) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->magenta) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.12.0->magenta) (3.6.1)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7->bokeh>=0.12.0->magenta) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: llvmlite>=0.25.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa>=0.6.2->magenta) (0.26.0)\n",
            "Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->IPython->magenta) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->magenta) (0.1.7)\n",
            "Requirement already satisfied, skipping upgrade: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->IPython->magenta) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow>=1.12.0->magenta) (3.0.1)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow>=1.12.0->magenta) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow>=1.12.0->magenta) (2.8.0)\n",
            "Installing collected packages: magenta, pyfluidsynth\n",
            "  Found existing installation: magenta 0.3.19\n",
            "    Uninstalling magenta-0.3.19:\n",
            "      Successfully uninstalled magenta-0.3.19\n",
            "Successfully installed magenta-0.4.0 pyfluidsynth-1.2.5\n",
            "Copying gs://download.magenta.tensorflow.org/soundfonts/Yamaha-C5-Salamander-JNv5.1.sf2...\n",
            "/ [1/1 files][591.9 MiB/591.9 MiB] 100% Done  46.7 MiB/s ETA 00:00:00           \n",
            "Operation completed over 1 objects/591.9 MiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sBTqgAjE7zGy",
        "colab_type": "code",
        "outputId": "dce53cef-4dce-49f6-c55c-ae477c2ce3a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10165
        }
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "\"\"\"TensorFlow MNIST AutoEncoder\n",
        "\n",
        "This is my attempt to write the autoencoder for MNIST by Andrej Karpathy using \n",
        "ConvNetJS in TensorFlow. Mostly to get some more experience working in \n",
        "Tensorflow.\n",
        "\n",
        "Sources:\n",
        "    - http://cs.stanford.edu/people/karpathy/convnetjs/demo/autoencoder.html\n",
        "    - https://www.tensorflow.org/get_started/mnist/pros\n",
        "\n",
        "Author: Gertjan van den Burg\n",
        "Date: Thu Oct 26 16:49:29 CEST 2017\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from magenta.models.image_stylization.image_utils import form_image_grid\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "BATCH_SIZE = 50\n",
        "GRID_ROWS = 5\n",
        "GRID_COLS = 10\n",
        "USE_RELU = True\n",
        "\n",
        "\n",
        "def weight_variable(shape):\n",
        "    # From the mnist tutorial\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "\n",
        "def fc_layer(previous, input_size, output_size):\n",
        "    W = weight_variable([input_size, output_size])\n",
        "    b = bias_variable([output_size])\n",
        "    return tf.matmul(previous, W) + b\n",
        "\n",
        "\n",
        "def autoencoder(x):\n",
        "    # first fully connected layer with 50 neurons using tanh activation\n",
        "    l1 = tf.nn.tanh(fc_layer(x, 28*28, 50))\n",
        "    # second fully connected layer with 50 neurons using tanh activation\n",
        "    l2 = tf.nn.tanh(fc_layer(l1, 50, 50))\n",
        "    # third fully connected layer with 2 neurons\n",
        "    l3 = fc_layer(l2, 50, 2)\n",
        "    # fourth fully connected layer with 50 neurons and tanh activation\n",
        "    l4 = tf.nn.tanh(fc_layer(l3, 2, 50))\n",
        "    # fifth fully connected layer with 50 neurons and tanh activation\n",
        "    l5 = tf.nn.tanh(fc_layer(l4, 50, 50))\n",
        "    # readout layer\n",
        "    if USE_RELU:\n",
        "        out = tf.nn.relu(fc_layer(l5, 50, 28*28))\n",
        "    else:\n",
        "        out = fc_layer(l5, 50, 28*28)\n",
        "    # let's use an l2 loss on the output image\n",
        "    loss = tf.reduce_mean(tf.squared_difference(x, out))\n",
        "    # http://testpy.hatenablog.com/entry/2016/11/27/035033\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(out, 1), tf.argmax(x, 1)), tf.float32))\n",
        "    # https://qiita.com/mytk0u0/items/d11ce0c7393e96149fa6\n",
        "    # accuracy, update_op = tf.metrics.accuracy(x, out)\n",
        "    return loss, out, l3, accuracy\n",
        "\n",
        "\n",
        "def layer_grid_summary(name, var, image_dims):\n",
        "    prod = np.prod(image_dims)\n",
        "    grid = form_image_grid(tf.reshape(var, [BATCH_SIZE, prod]), [GRID_ROWS, \n",
        "        GRID_COLS], image_dims, 1)\n",
        "    return tf.summary.image(name, grid)\n",
        "\n",
        "\n",
        "def create_summaries(loss, x, latent, output, accuracy):\n",
        "    writer = tf.summary.FileWriter(\"./logs\")\n",
        "    tf.summary.scalar(\"Loss\", loss)\n",
        "    tf.summary.scalar(\"Accuracy\", accuracy)\n",
        "    layer_grid_summary(\"Input\", x, [28, 28])\n",
        "    layer_grid_summary(\"Encoder\", latent, [2, 1])\n",
        "    layer_grid_summary(\"Output\", output, [28, 28])\n",
        "    return writer, tf.summary.merge_all()\n",
        "\n",
        "\n",
        "def make_image(name, var, image_dims):\n",
        "    prod = np.prod(image_dims)\n",
        "    grid = form_image_grid(tf.reshape(var, [BATCH_SIZE, prod]), [GRID_ROWS, \n",
        "        GRID_COLS], image_dims, 1)\n",
        "    s_grid = tf.squeeze(grid, axis=0)\n",
        "\n",
        "    # This reproduces the code in: tensorflow/core/kernels/summary_image_op.cc\n",
        "    im_min = tf.reduce_min(s_grid)\n",
        "    im_max = tf.reduce_max(s_grid)\n",
        "\n",
        "    kZeroThreshold = tf.constant(1e-6)\n",
        "    max_val = tf.maximum(tf.abs(im_min), tf.abs(im_max))\n",
        "\n",
        "    offset = tf.cond(\n",
        "            im_min < tf.constant(0.0),\n",
        "            lambda: tf.constant(128.0),\n",
        "            lambda: tf.constant(0.0)\n",
        "            )\n",
        "    scale = tf.cond(\n",
        "            im_min < tf.constant(0.0),\n",
        "            lambda: tf.cond(\n",
        "                max_val < kZeroThreshold,\n",
        "                lambda: tf.constant(0.0),\n",
        "                lambda: tf.div(127.0, max_val)\n",
        "                ),\n",
        "            lambda: tf.cond(\n",
        "                im_max < kZeroThreshold,\n",
        "                lambda: tf.constant(0.0),\n",
        "                lambda: tf.div(255.0, im_max)\n",
        "                )\n",
        "            )\n",
        "    s_grid = tf.cast(tf.add(tf.multiply(s_grid, scale), offset), tf.uint8)\n",
        "    enc = tf.image.encode_jpeg(s_grid)\n",
        "\n",
        "    fwrite = tf.write_file(name, enc)\n",
        "    return fwrite\n",
        "\n",
        "\n",
        "def main():\n",
        "    # initialize the data\n",
        "    mnist = input_data.read_data_sets('/tmp/MNIST_data')\n",
        "\n",
        "    # placeholders for the images\n",
        "    x = tf.placeholder(tf.float32, shape=[None, 784])\n",
        "\n",
        "    # build the model\n",
        "    loss, output, latent, accuracy = autoencoder(x)\n",
        "\n",
        "    # and we use the Adam Optimizer for training\n",
        "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
        "\n",
        "    # We want to use Tensorboard to visualize some stuff\n",
        "    writer, summary_op = create_summaries(loss, x, latent, output, accuracy)\n",
        "\n",
        "    first_batch = mnist.train.next_batch(BATCH_SIZE)\n",
        "\n",
        "    # Run the training loop\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        sess.run(make_image(\"images/input.jpg\", x, [28, 28]), feed_dict={x : \n",
        "            first_batch[0]})\n",
        "        for i in range(int(200001)):\n",
        "            batch = mnist.train.next_batch(BATCH_SIZE)\n",
        "            feed = {x : batch[0]}\n",
        "            if i % 500 == 0:\n",
        "                summary, train_loss, acc = sess.run([summary_op, loss, accuracy], \n",
        "                        feed_dict=feed)\n",
        "                print(\"step %d, training loss: %g acc:%g\" % (i, train_loss, acc))\n",
        "\n",
        "                writer.add_summary(summary, i)\n",
        "                writer.flush()\n",
        "\n",
        "            if i % 1000 == 0:\n",
        "                sess.run(make_image(\"images/output_%06i.jpg\" % i, output, [28, \n",
        "                    28]), feed_dict={x : first_batch[0]})\n",
        "\n",
        "            train_step.run(feed_dict=feed)\n",
        "\n",
        "        # Save latent space\n",
        "        pred = sess.run(latent, feed_dict={x : mnist.test._images})\n",
        "        pred = np.asarray(pred)\n",
        "        pred = np.reshape(pred, (mnist.test._num_examples, 2))\n",
        "        labels = np.reshape(mnist.test._labels, (mnist.test._num_examples, 1))\n",
        "        pred = np.hstack((pred, labels))\n",
        "        if USE_RELU:\n",
        "            fname = \"latent_relu.csv\"\n",
        "        else:\n",
        "            fname = \"latent_default.csv\"\n",
        "        np.savetxt(fname, pred)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-7219a712fd06>:128: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting /tmp/MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting /tmp/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "step 0, training loss: 0.101742 acc:0\n",
            "step 500, training loss: 0.0672742 acc:0\n",
            "step 1000, training loss: 0.060028 acc:0.02\n",
            "step 1500, training loss: 0.0560907 acc:0.02\n",
            "step 2000, training loss: 0.0547143 acc:0.02\n",
            "step 2500, training loss: 0.0520989 acc:0.04\n",
            "step 3000, training loss: 0.0549931 acc:0.02\n",
            "step 3500, training loss: 0.0522342 acc:0.02\n",
            "step 4000, training loss: 0.0557859 acc:0.04\n",
            "step 4500, training loss: 0.0598539 acc:0.02\n",
            "step 5000, training loss: 0.0563242 acc:0\n",
            "step 5500, training loss: 0.0561486 acc:0.04\n",
            "step 6000, training loss: 0.047853 acc:0.06\n",
            "step 6500, training loss: 0.0485488 acc:0\n",
            "step 7000, training loss: 0.0504724 acc:0\n",
            "step 7500, training loss: 0.0521193 acc:0\n",
            "step 8000, training loss: 0.0474973 acc:0\n",
            "step 8500, training loss: 0.0506225 acc:0.02\n",
            "step 9000, training loss: 0.0530216 acc:0\n",
            "step 9500, training loss: 0.0473818 acc:0\n",
            "step 10000, training loss: 0.045429 acc:0.02\n",
            "step 10500, training loss: 0.0509291 acc:0\n",
            "step 11000, training loss: 0.0479249 acc:0\n",
            "step 11500, training loss: 0.0460584 acc:0.02\n",
            "step 12000, training loss: 0.0515108 acc:0.04\n",
            "step 12500, training loss: 0.0417455 acc:0.02\n",
            "step 13000, training loss: 0.049993 acc:0\n",
            "step 13500, training loss: 0.0498681 acc:0\n",
            "step 14000, training loss: 0.0472845 acc:0\n",
            "step 14500, training loss: 0.0470237 acc:0.02\n",
            "step 15000, training loss: 0.0499254 acc:0\n",
            "step 15500, training loss: 0.0470122 acc:0.02\n",
            "step 16000, training loss: 0.0511422 acc:0\n",
            "step 16500, training loss: 0.045694 acc:0\n",
            "step 17000, training loss: 0.0435124 acc:0.02\n",
            "step 17500, training loss: 0.0453955 acc:0.02\n",
            "step 18000, training loss: 0.0430855 acc:0.06\n",
            "step 18500, training loss: 0.0479951 acc:0\n",
            "step 19000, training loss: 0.0478459 acc:0\n",
            "step 19500, training loss: 0.047046 acc:0\n",
            "step 20000, training loss: 0.0494627 acc:0\n",
            "step 20500, training loss: 0.0453287 acc:0.04\n",
            "step 21000, training loss: 0.0461757 acc:0.02\n",
            "step 21500, training loss: 0.0491381 acc:0\n",
            "step 22000, training loss: 0.0471307 acc:0\n",
            "step 22500, training loss: 0.0504889 acc:0\n",
            "step 23000, training loss: 0.0414426 acc:0\n",
            "step 23500, training loss: 0.0470879 acc:0\n",
            "step 24000, training loss: 0.0495709 acc:0.02\n",
            "step 24500, training loss: 0.0471315 acc:0\n",
            "step 25000, training loss: 0.0431893 acc:0\n",
            "step 25500, training loss: 0.0489946 acc:0\n",
            "step 26000, training loss: 0.0484966 acc:0.02\n",
            "step 26500, training loss: 0.0525103 acc:0\n",
            "step 27000, training loss: 0.0426248 acc:0.04\n",
            "step 27500, training loss: 0.039916 acc:0.02\n",
            "step 28000, training loss: 0.0426137 acc:0\n",
            "step 28500, training loss: 0.0477282 acc:0\n",
            "step 29000, training loss: 0.0469789 acc:0.02\n",
            "step 29500, training loss: 0.0404976 acc:0.02\n",
            "step 30000, training loss: 0.0447525 acc:0.02\n",
            "step 30500, training loss: 0.0445102 acc:0\n",
            "step 31000, training loss: 0.0444452 acc:0\n",
            "step 31500, training loss: 0.0457752 acc:0\n",
            "step 32000, training loss: 0.039199 acc:0\n",
            "step 32500, training loss: 0.0483772 acc:0.02\n",
            "step 33000, training loss: 0.0452211 acc:0.02\n",
            "step 33500, training loss: 0.0401593 acc:0.02\n",
            "step 34000, training loss: 0.0430296 acc:0.04\n",
            "step 34500, training loss: 0.0421895 acc:0.02\n",
            "step 35000, training loss: 0.0447855 acc:0\n",
            "step 35500, training loss: 0.040313 acc:0\n",
            "step 36000, training loss: 0.0422752 acc:0\n",
            "step 36500, training loss: 0.0462992 acc:0\n",
            "step 37000, training loss: 0.0421881 acc:0\n",
            "step 37500, training loss: 0.040607 acc:0\n",
            "step 38000, training loss: 0.0450974 acc:0.02\n",
            "step 38500, training loss: 0.0421018 acc:0.02\n",
            "step 39000, training loss: 0.0441689 acc:0\n",
            "step 39500, training loss: 0.0453751 acc:0.02\n",
            "step 40000, training loss: 0.0459486 acc:0\n",
            "step 40500, training loss: 0.0450103 acc:0\n",
            "step 41000, training loss: 0.0439898 acc:0\n",
            "step 41500, training loss: 0.0423851 acc:0\n",
            "step 42000, training loss: 0.0442912 acc:0\n",
            "step 42500, training loss: 0.0409075 acc:0.04\n",
            "step 43000, training loss: 0.040186 acc:0\n",
            "step 43500, training loss: 0.0401026 acc:0.02\n",
            "step 44000, training loss: 0.0407093 acc:0.02\n",
            "step 44500, training loss: 0.0425949 acc:0\n",
            "step 45000, training loss: 0.0439044 acc:0\n",
            "step 45500, training loss: 0.0461591 acc:0\n",
            "step 46000, training loss: 0.0423993 acc:0.02\n",
            "step 46500, training loss: 0.0423627 acc:0.04\n",
            "step 47000, training loss: 0.0403451 acc:0\n",
            "step 47500, training loss: 0.0429416 acc:0.02\n",
            "step 48000, training loss: 0.0455465 acc:0\n",
            "step 48500, training loss: 0.0428765 acc:0\n",
            "step 49000, training loss: 0.0410686 acc:0.02\n",
            "step 49500, training loss: 0.0381996 acc:0.02\n",
            "step 50000, training loss: 0.0371178 acc:0\n",
            "step 50500, training loss: 0.0393077 acc:0\n",
            "step 51000, training loss: 0.038552 acc:0\n",
            "step 51500, training loss: 0.0428861 acc:0.02\n",
            "step 52000, training loss: 0.0403086 acc:0.02\n",
            "step 52500, training loss: 0.0394859 acc:0.04\n",
            "step 53000, training loss: 0.0433389 acc:0\n",
            "step 53500, training loss: 0.0394145 acc:0.02\n",
            "step 54000, training loss: 0.0420975 acc:0.02\n",
            "step 54500, training loss: 0.0392786 acc:0\n",
            "step 55000, training loss: 0.0412435 acc:0.02\n",
            "step 55500, training loss: 0.0356748 acc:0\n",
            "step 56000, training loss: 0.0447908 acc:0.02\n",
            "step 56500, training loss: 0.041121 acc:0\n",
            "step 57000, training loss: 0.0416591 acc:0\n",
            "step 57500, training loss: 0.0405933 acc:0\n",
            "step 58000, training loss: 0.0422296 acc:0\n",
            "step 58500, training loss: 0.0392047 acc:0\n",
            "step 59000, training loss: 0.0420792 acc:0.04\n",
            "step 59500, training loss: 0.038065 acc:0.04\n",
            "step 60000, training loss: 0.0431246 acc:0.04\n",
            "step 60500, training loss: 0.0386306 acc:0.02\n",
            "step 61000, training loss: 0.0403372 acc:0.04\n",
            "step 61500, training loss: 0.0423906 acc:0\n",
            "step 62000, training loss: 0.0391532 acc:0\n",
            "step 62500, training loss: 0.0442225 acc:0\n",
            "step 63000, training loss: 0.0434298 acc:0.02\n",
            "step 63500, training loss: 0.0420156 acc:0.02\n",
            "step 64000, training loss: 0.0417871 acc:0.02\n",
            "step 64500, training loss: 0.0405298 acc:0\n",
            "step 65000, training loss: 0.0425307 acc:0\n",
            "step 65500, training loss: 0.0382577 acc:0\n",
            "step 66000, training loss: 0.0459178 acc:0\n",
            "step 66500, training loss: 0.0363065 acc:0\n",
            "step 67000, training loss: 0.0415999 acc:0.02\n",
            "step 67500, training loss: 0.041428 acc:0.04\n",
            "step 68000, training loss: 0.0416874 acc:0.02\n",
            "step 68500, training loss: 0.038696 acc:0.08\n",
            "step 69000, training loss: 0.0366802 acc:0\n",
            "step 69500, training loss: 0.0367093 acc:0.02\n",
            "step 70000, training loss: 0.0432062 acc:0.02\n",
            "step 70500, training loss: 0.0415005 acc:0\n",
            "step 71000, training loss: 0.040591 acc:0\n",
            "step 71500, training loss: 0.0411905 acc:0\n",
            "step 72000, training loss: 0.0385577 acc:0\n",
            "step 72500, training loss: 0.0394003 acc:0.02\n",
            "step 73000, training loss: 0.0365691 acc:0\n",
            "step 73500, training loss: 0.0441067 acc:0\n",
            "step 74000, training loss: 0.0405724 acc:0\n",
            "step 74500, training loss: 0.040227 acc:0.02\n",
            "step 75000, training loss: 0.044321 acc:0\n",
            "step 75500, training loss: 0.0425112 acc:0\n",
            "step 76000, training loss: 0.0391701 acc:0.02\n",
            "step 76500, training loss: 0.0347859 acc:0.04\n",
            "step 77000, training loss: 0.0427466 acc:0\n",
            "step 77500, training loss: 0.0397067 acc:0.02\n",
            "step 78000, training loss: 0.04239 acc:0.04\n",
            "step 78500, training loss: 0.0450383 acc:0\n",
            "step 79000, training loss: 0.0425587 acc:0.02\n",
            "step 79500, training loss: 0.0404377 acc:0.02\n",
            "step 80000, training loss: 0.0380906 acc:0\n",
            "step 80500, training loss: 0.0424171 acc:0.02\n",
            "step 81000, training loss: 0.0392161 acc:0\n",
            "step 81500, training loss: 0.0409892 acc:0\n",
            "step 82000, training loss: 0.0386501 acc:0\n",
            "step 82500, training loss: 0.0421267 acc:0.02\n",
            "step 83000, training loss: 0.0397849 acc:0\n",
            "step 83500, training loss: 0.0394703 acc:0\n",
            "step 84000, training loss: 0.0340616 acc:0\n",
            "step 84500, training loss: 0.0381814 acc:0\n",
            "step 85000, training loss: 0.0406431 acc:0\n",
            "step 85500, training loss: 0.036357 acc:0\n",
            "step 86000, training loss: 0.0384922 acc:0\n",
            "step 86500, training loss: 0.0395211 acc:0\n",
            "step 87000, training loss: 0.0353238 acc:0.04\n",
            "step 87500, training loss: 0.0432044 acc:0\n",
            "step 88000, training loss: 0.0347652 acc:0.04\n",
            "step 88500, training loss: 0.0395569 acc:0\n",
            "step 89000, training loss: 0.0405596 acc:0.02\n",
            "step 89500, training loss: 0.0347403 acc:0\n",
            "step 90000, training loss: 0.0426464 acc:0\n",
            "step 90500, training loss: 0.0400967 acc:0\n",
            "step 91000, training loss: 0.0398341 acc:0.02\n",
            "step 91500, training loss: 0.0413353 acc:0\n",
            "step 92000, training loss: 0.0416293 acc:0.02\n",
            "step 92500, training loss: 0.0372277 acc:0\n",
            "step 93000, training loss: 0.0415696 acc:0.02\n",
            "step 93500, training loss: 0.035682 acc:0\n",
            "step 94000, training loss: 0.044332 acc:0.02\n",
            "step 94500, training loss: 0.040576 acc:0\n",
            "step 95000, training loss: 0.0440791 acc:0\n",
            "step 95500, training loss: 0.0388142 acc:0\n",
            "step 96000, training loss: 0.0391883 acc:0\n",
            "step 96500, training loss: 0.0388213 acc:0.04\n",
            "step 97000, training loss: 0.0380016 acc:0\n",
            "step 97500, training loss: 0.0378029 acc:0.02\n",
            "step 98000, training loss: 0.0377402 acc:0\n",
            "step 98500, training loss: 0.0418404 acc:0.02\n",
            "step 99000, training loss: 0.040733 acc:0\n",
            "step 99500, training loss: 0.0417101 acc:0.02\n",
            "step 100000, training loss: 0.0387652 acc:0.04\n",
            "step 100500, training loss: 0.0392561 acc:0\n",
            "step 101000, training loss: 0.0376564 acc:0\n",
            "step 101500, training loss: 0.0349523 acc:0\n",
            "step 102000, training loss: 0.0362971 acc:0.02\n",
            "step 102500, training loss: 0.0391697 acc:0.02\n",
            "step 103000, training loss: 0.0417303 acc:0.02\n",
            "step 103500, training loss: 0.0391434 acc:0.04\n",
            "step 104000, training loss: 0.0388705 acc:0\n",
            "step 104500, training loss: 0.0398878 acc:0\n",
            "step 105000, training loss: 0.0385592 acc:0.02\n",
            "step 105500, training loss: 0.0423911 acc:0\n",
            "step 106000, training loss: 0.0379929 acc:0.02\n",
            "step 106500, training loss: 0.0366952 acc:0\n",
            "step 107000, training loss: 0.0421742 acc:0.02\n",
            "step 107500, training loss: 0.035748 acc:0\n",
            "step 108000, training loss: 0.0444251 acc:0.04\n",
            "step 108500, training loss: 0.0352774 acc:0\n",
            "step 109000, training loss: 0.042104 acc:0\n",
            "step 109500, training loss: 0.0385882 acc:0\n",
            "step 110000, training loss: 0.0402012 acc:0\n",
            "step 110500, training loss: 0.037241 acc:0\n",
            "step 111000, training loss: 0.0389469 acc:0.04\n",
            "step 111500, training loss: 0.0415824 acc:0\n",
            "step 112000, training loss: 0.0375811 acc:0.02\n",
            "step 112500, training loss: 0.0342599 acc:0\n",
            "step 113000, training loss: 0.0367876 acc:0\n",
            "step 113500, training loss: 0.0396191 acc:0\n",
            "step 114000, training loss: 0.0379909 acc:0.02\n",
            "step 114500, training loss: 0.0368912 acc:0\n",
            "step 115000, training loss: 0.0392299 acc:0.02\n",
            "step 115500, training loss: 0.0389267 acc:0\n",
            "step 116000, training loss: 0.0374542 acc:0\n",
            "step 116500, training loss: 0.0410957 acc:0\n",
            "step 117000, training loss: 0.0429588 acc:0\n",
            "step 117500, training loss: 0.0377287 acc:0\n",
            "step 118000, training loss: 0.0385736 acc:0\n",
            "step 118500, training loss: 0.0428142 acc:0.02\n",
            "step 119000, training loss: 0.0400097 acc:0.02\n",
            "step 119500, training loss: 0.035537 acc:0\n",
            "step 120000, training loss: 0.0377893 acc:0\n",
            "step 120500, training loss: 0.0349971 acc:0\n",
            "step 121000, training loss: 0.0403322 acc:0\n",
            "step 121500, training loss: 0.0384084 acc:0\n",
            "step 122000, training loss: 0.0331428 acc:0\n",
            "step 122500, training loss: 0.032848 acc:0\n",
            "step 123000, training loss: 0.0414858 acc:0\n",
            "step 123500, training loss: 0.0364488 acc:0\n",
            "step 124000, training loss: 0.0351161 acc:0.02\n",
            "step 124500, training loss: 0.0368754 acc:0\n",
            "step 125000, training loss: 0.0417009 acc:0\n",
            "step 125500, training loss: 0.03763 acc:0\n",
            "step 126000, training loss: 0.0372492 acc:0.02\n",
            "step 126500, training loss: 0.0399534 acc:0\n",
            "step 127000, training loss: 0.0411969 acc:0\n",
            "step 127500, training loss: 0.0397243 acc:0.04\n",
            "step 128000, training loss: 0.0394435 acc:0\n",
            "step 128500, training loss: 0.0445574 acc:0\n",
            "step 129000, training loss: 0.0393461 acc:0\n",
            "step 129500, training loss: 0.0447979 acc:0.02\n",
            "step 130000, training loss: 0.0360003 acc:0\n",
            "step 130500, training loss: 0.0355456 acc:0\n",
            "step 131000, training loss: 0.0365237 acc:0\n",
            "step 131500, training loss: 0.0408209 acc:0.02\n",
            "step 132000, training loss: 0.0413544 acc:0\n",
            "step 132500, training loss: 0.0351091 acc:0.02\n",
            "step 133000, training loss: 0.0404121 acc:0.02\n",
            "step 133500, training loss: 0.0383598 acc:0.02\n",
            "step 134000, training loss: 0.0386062 acc:0\n",
            "step 134500, training loss: 0.03801 acc:0\n",
            "step 135000, training loss: 0.0376542 acc:0.04\n",
            "step 135500, training loss: 0.0366751 acc:0\n",
            "step 136000, training loss: 0.0376172 acc:0.04\n",
            "step 136500, training loss: 0.0419701 acc:0\n",
            "step 137000, training loss: 0.0384602 acc:0.02\n",
            "step 137500, training loss: 0.032097 acc:0.04\n",
            "step 138000, training loss: 0.0355521 acc:0\n",
            "step 138500, training loss: 0.0359713 acc:0\n",
            "step 139000, training loss: 0.0381266 acc:0.02\n",
            "step 139500, training loss: 0.0349124 acc:0.02\n",
            "step 140000, training loss: 0.0407815 acc:0.06\n",
            "step 140500, training loss: 0.0380534 acc:0\n",
            "step 141000, training loss: 0.0357899 acc:0.04\n",
            "step 141500, training loss: 0.0362562 acc:0.02\n",
            "step 142000, training loss: 0.0442828 acc:0\n",
            "step 142500, training loss: 0.0379155 acc:0\n",
            "step 143000, training loss: 0.0358009 acc:0\n",
            "step 143500, training loss: 0.0365787 acc:0\n",
            "step 144000, training loss: 0.0375834 acc:0\n",
            "step 144500, training loss: 0.037244 acc:0.02\n",
            "step 145000, training loss: 0.036114 acc:0\n",
            "step 145500, training loss: 0.037838 acc:0\n",
            "step 146000, training loss: 0.0385974 acc:0.04\n",
            "step 146500, training loss: 0.0407598 acc:0.02\n",
            "step 147000, training loss: 0.0345546 acc:0.02\n",
            "step 147500, training loss: 0.035553 acc:0\n",
            "step 148000, training loss: 0.0359648 acc:0\n",
            "step 148500, training loss: 0.037605 acc:0.04\n",
            "step 149000, training loss: 0.0399039 acc:0\n",
            "step 149500, training loss: 0.0384753 acc:0\n",
            "step 150000, training loss: 0.0355843 acc:0\n",
            "step 150500, training loss: 0.0382089 acc:0\n",
            "step 151000, training loss: 0.0373971 acc:0.02\n",
            "step 151500, training loss: 0.0349469 acc:0.02\n",
            "step 152000, training loss: 0.0353033 acc:0\n",
            "step 152500, training loss: 0.0384243 acc:0\n",
            "step 153000, training loss: 0.0330718 acc:0\n",
            "step 153500, training loss: 0.0373467 acc:0.02\n",
            "step 154000, training loss: 0.0355373 acc:0.02\n",
            "step 154500, training loss: 0.0361532 acc:0.02\n",
            "step 155000, training loss: 0.0377551 acc:0\n",
            "step 155500, training loss: 0.0398672 acc:0\n",
            "step 156000, training loss: 0.0370299 acc:0\n",
            "step 156500, training loss: 0.0390766 acc:0\n",
            "step 157000, training loss: 0.0371382 acc:0\n",
            "step 157500, training loss: 0.0332835 acc:0\n",
            "step 158000, training loss: 0.0362383 acc:0\n",
            "step 158500, training loss: 0.0381298 acc:0.04\n",
            "step 159000, training loss: 0.03966 acc:0\n",
            "step 159500, training loss: 0.0412378 acc:0\n",
            "step 160000, training loss: 0.038799 acc:0\n",
            "step 160500, training loss: 0.0368593 acc:0.02\n",
            "step 161000, training loss: 0.0395768 acc:0.02\n",
            "step 161500, training loss: 0.0395302 acc:0\n",
            "step 162000, training loss: 0.0363564 acc:0.04\n",
            "step 162500, training loss: 0.0346937 acc:0\n",
            "step 163000, training loss: 0.0370894 acc:0.02\n",
            "step 163500, training loss: 0.0342991 acc:0.02\n",
            "step 164000, training loss: 0.0379793 acc:0\n",
            "step 164500, training loss: 0.039214 acc:0\n",
            "step 165000, training loss: 0.0359174 acc:0\n",
            "step 165500, training loss: 0.0436251 acc:0\n",
            "step 166000, training loss: 0.0372565 acc:0\n",
            "step 166500, training loss: 0.0417847 acc:0.04\n",
            "step 167000, training loss: 0.0361101 acc:0.04\n",
            "step 167500, training loss: 0.0368469 acc:0.04\n",
            "step 168000, training loss: 0.0388908 acc:0.02\n",
            "step 168500, training loss: 0.0339035 acc:0.02\n",
            "step 169000, training loss: 0.0436078 acc:0\n",
            "step 169500, training loss: 0.0391201 acc:0.02\n",
            "step 170000, training loss: 0.0421138 acc:0.02\n",
            "step 170500, training loss: 0.0356241 acc:0\n",
            "step 171000, training loss: 0.0372583 acc:0.06\n",
            "step 171500, training loss: 0.0382797 acc:0.02\n",
            "step 172000, training loss: 0.0389686 acc:0.02\n",
            "step 172500, training loss: 0.0368684 acc:0\n",
            "step 173000, training loss: 0.0360729 acc:0.04\n",
            "step 173500, training loss: 0.0381287 acc:0\n",
            "step 174000, training loss: 0.0395364 acc:0\n",
            "step 174500, training loss: 0.0403488 acc:0\n",
            "step 175000, training loss: 0.0363753 acc:0.02\n",
            "step 175500, training loss: 0.0313549 acc:0\n",
            "step 176000, training loss: 0.0376212 acc:0\n",
            "step 176500, training loss: 0.0389246 acc:0\n",
            "step 177000, training loss: 0.0402742 acc:0\n",
            "step 177500, training loss: 0.0414707 acc:0\n",
            "step 178000, training loss: 0.0407089 acc:0.04\n",
            "step 178500, training loss: 0.0369917 acc:0.02\n",
            "step 179000, training loss: 0.0400179 acc:0\n",
            "step 179500, training loss: 0.0346682 acc:0.02\n",
            "step 180000, training loss: 0.0353668 acc:0\n",
            "step 180500, training loss: 0.0366807 acc:0\n",
            "step 181000, training loss: 0.0370981 acc:0\n",
            "step 181500, training loss: 0.0352272 acc:0\n",
            "step 182000, training loss: 0.0430725 acc:0\n",
            "step 182500, training loss: 0.0361935 acc:0\n",
            "step 183000, training loss: 0.0354739 acc:0\n",
            "step 183500, training loss: 0.0368214 acc:0\n",
            "step 184000, training loss: 0.0369633 acc:0.02\n",
            "step 184500, training loss: 0.037225 acc:0.02\n",
            "step 185000, training loss: 0.0395256 acc:0\n",
            "step 185500, training loss: 0.0405715 acc:0.02\n",
            "step 186000, training loss: 0.0375389 acc:0.02\n",
            "step 186500, training loss: 0.0381578 acc:0.04\n",
            "step 187000, training loss: 0.0396718 acc:0\n",
            "step 187500, training loss: 0.0372974 acc:0\n",
            "step 188000, training loss: 0.0380738 acc:0.02\n",
            "step 188500, training loss: 0.0346616 acc:0.04\n",
            "step 189000, training loss: 0.0369167 acc:0.02\n",
            "step 189500, training loss: 0.0360544 acc:0\n",
            "step 190000, training loss: 0.0365506 acc:0.02\n",
            "step 190500, training loss: 0.0345465 acc:0.02\n",
            "step 191000, training loss: 0.0362064 acc:0\n",
            "step 191500, training loss: 0.0341075 acc:0\n",
            "step 192000, training loss: 0.0370097 acc:0\n",
            "step 192500, training loss: 0.0353675 acc:0\n",
            "step 193000, training loss: 0.0380435 acc:0.04\n",
            "step 193500, training loss: 0.0330079 acc:0\n",
            "step 194000, training loss: 0.0342591 acc:0\n",
            "step 194500, training loss: 0.0400774 acc:0\n",
            "step 195000, training loss: 0.0359704 acc:0\n",
            "step 195500, training loss: 0.0369456 acc:0\n",
            "step 196000, training loss: 0.0369597 acc:0.02\n",
            "step 196500, training loss: 0.0381547 acc:0\n",
            "step 197000, training loss: 0.0430525 acc:0\n",
            "step 197500, training loss: 0.0395461 acc:0\n",
            "step 198000, training loss: 0.0376733 acc:0.02\n",
            "step 198500, training loss: 0.0356403 acc:0\n",
            "step 199000, training loss: 0.0354888 acc:0\n",
            "step 199500, training loss: 0.0347397 acc:0\n",
            "step 200000, training loss: 0.0346684 acc:0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}