{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoencoderForThesis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s1230038/autoencoder/blob/master/AutoencoderForThesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "42k_7hqZp7yx",
        "colab_type": "code",
        "outputId": "08d1baa3-b6d1-4bd7-a280-667d60a9f707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "rm *txt *h5 *png *zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '*txt': No such file or directory\n",
            "rm: cannot remove '*h5': No such file or directory\n",
            "rm: cannot remove '*png': No such file or directory\n",
            "rm: cannot remove '*zip': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d5NaL7in5Jc1",
        "colab_type": "code",
        "outputId": "1cb3ab10-02e8-4dac-ab49-6632034b0c90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "# Simple Autoencoder using Other Loss Function\n",
        "# Original: https://elix-tech.github.io/ja/2016/07/17/autoencoder.html\n",
        "# https://colab.research.google.com/drive/1Z_d8APkMUDwXDQIg3OI7E13vH8IZhusM?authuser=1#scrollTo=WmBfOis_mWCH\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras import backend as K\n",
        "from keras import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import losses\n",
        "from tensorflow.python import debug as tf_debug\n",
        "from itertools import product\n",
        "from google.colab import files\n",
        "from keras import optimizers\n",
        "from keras import constraints as cst\n",
        "\n",
        "# imititing mean_squared_error():\n",
        "# é’ã‚¤ãƒ«ã‚« P.57ã§ã¯äºŒä¹—èª¤å·®ã®ç·å’Œã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãŒã€ã“ã‚Œã‚’Kerasã§å®Ÿè£…ã™ã‚‹å ´åˆã¯å¹³å‡äºŒä¹—èª¤å·®ï¼ˆmean_squared_error)\n",
        "# ã‚’loss ã«æ¸¡ã™å¿…è¦ãŒã‚ã‚‹ã€‚ãªãœãªã‚‰ã€Kerasã¯ãƒŸãƒ‹ãƒãƒƒãƒã§fit()ã‚’è¨ˆç®—ã™ã‚‹ã‹ã‚‰ã§ã‚ã‚‹ã€‚é’ã‚¤ãƒ«ã‚« P.27å‚ç…§ã€‚\n",
        "# ğ’™ Ì‚_ğ‘› :y_pred,  ğ’™_ğ‘› : y_true, because x_n is training data which means label.\n",
        "def i_divergence1(y_true, y_pred):\n",
        "  y_true = K.clip(y_true, K.epsilon(), 1)\n",
        "  y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
        "  return K.mean(y_pred * K.log(y_pred / y_true) - y_pred + y_true, axis=-1)\n",
        "\n",
        "def i_divergence2(y_true, y_pred):\n",
        "  y_true = K.clip(y_true, K.epsilon(), 1)\n",
        "  y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
        "  return K.mean(y_true * K.log(y_true / y_pred) - y_true + y_pred, axis=-1)\n",
        "\n",
        "def is_divergence1(y_true, y_pred):\n",
        "  y_true = K.clip(y_true, K.epsilon(), 1)\n",
        "  y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
        "  return K.mean( (y_pred / y_true) - K.log(y_pred / y_true) - 1, axis=-1)\n",
        "\n",
        "def is_divergence2(y_true, y_pred):\n",
        "  y_true = K.clip(y_true, K.epsilon(), 1)\n",
        "  y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
        "  return K.mean( (y_true / y_pred) - K.log(y_true / y_pred) - 1, axis=-1)\n",
        "\n",
        "# https://qiita.com/hiroyuki827/items/213146d551a6e2227810\n",
        "def plot_history_loss(np_loss, np_vloss, name):\n",
        "    # Plot the loss in the history\n",
        "    fig, axL = plt.subplots(figsize=(8,6), dpi=500) # ã‚°ãƒ©ãƒ•ã®è¡¨ç¤ºæº–å‚™\n",
        "    axL.plot(np_loss, label=\"loss for training\")\n",
        "    axL.plot(np_vloss, label=\"loss for validation\")\n",
        "    axL.set_title('model loss: ' + name)\n",
        "    axL.set_xlabel('epoch')\n",
        "    axL.set_ylabel('loss')\n",
        "    axL.legend(loc='upper right')\n",
        "    return fig\n",
        "\n",
        "\n",
        "epochs = 50\n",
        "# Download MNIST and standardize, learning\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# we will restrict domain of definition of the input data to the below expression with normalization of the input.\n",
        "x_train = x_train.astype('float32') / 255. # ç”»åƒãƒ‡ãƒ¼ã‚¿ã¯0ã‹ã‚‰1ã®å®Ÿæ•°å€¤ã‚’å–ã‚‹ã‚ˆã†ã«è¦æ ¼åŒ–\n",
        "x_test = x_test.astype('float32') / 255.   # {0,1}ã®äºŒå€¤ã§ã¯ãªãå®Ÿæ•°å€¤ã§ã‚ã‚‹ã“ã¨ã«æ³¨æ„\n",
        "# x_trainã¯ (60000, 28, 28) ã¨ã„ã†å½¢ã‚’ã—ã¦ã„ã¾ã™ãŒã€784æ¬¡å…ƒã®å…¥åŠ›ã«ãªã‚‹ã‚ˆã†ã« (60000, 784) ã«å¤‰å½¢\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "# hyper parameter combination\n",
        "'''\n",
        "lossfs = [losses.mean_squared_error, i_divergence1, i_divergence2, is_divergence1, is_divergence2]\n",
        "acts = [\"relu\", \"sigmoid\"]\n",
        "opzs = [\"SGD\", \"RMSprop\", \"Adagrad\", \"Adadelta\", \"Adam\", \"Adamax\", \"Nadam\"]\n",
        "'''\n",
        "lossfs = [losses.mean_squared_error, i_divergence1, i_divergence2]\n",
        "acts = [\"relu\", \"sigmoid\"]\n",
        "opzs = [\"SGD\", \"RMSprop\", \"Adagrad\", \"Adadelta\", \"Adam\", \"Adamax\", \"Nadam\"]\n",
        "# ------------------------------------------\n",
        "for loss, dact, optimizer in product(lossfs, acts, opzs):\n",
        "  file_prefix = (loss.__name__ + '_' + dact + '_' + optimizer + '_' )\n",
        "  print(\"start: \" + file_prefix )\n",
        "  encoding_dim = 32\n",
        "  input_img = Input(shape=(784,))\n",
        "  encoded = Dense(encoding_dim, activation='relu', \n",
        "                  kernel_constraint=cst.non_neg(), bias_constraint=cst.non_neg())(input_img) \n",
        "  decoded = Dense(784, activation=dact, kernel_constraint=cst.non_neg(), bias_constraint=cst.non_neg())(encoded)\n",
        "  autoencoder = Model(inputs=input_img, outputs=decoded) # https://medium.com/@zhuixiyou/userwarning-update-your-model-call-to-the-keras-2-api-8a6a5955daac\n",
        "  # autoencoderã§ã¯ã€æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã«ãƒ©ãƒ™ãƒ«ã‚’ä½¿ã‚ãªã„ãŸã‚accuracyã®è¨ˆç®—ã¯ä¸è¦ã€‚\n",
        "  autoencoder.compile(optimizer=optimizer, loss=loss)\n",
        "  # ------------------------------------------\n",
        "  fit = autoencoder.fit(x_train, x_train,\n",
        "                  epochs=epochs,\n",
        "                  batch_size=256,\n",
        "                  shuffle=True,\n",
        "                  verbose=0,\n",
        "                  validation_data=(x_test, x_test))\n",
        "\n",
        "  # lossã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜\n",
        "  loss_his = fit.history['loss']\n",
        "  vloss_his = fit.history['val_loss']\n",
        "  np_loss = np.array(loss_his)\n",
        "  np_vloss = np.array(vloss_his)\n",
        "  np.savetxt(file_prefix + \"loss_history.txt\",     np_loss,  delimiter=\",\")\n",
        "  np.savetxt(file_prefix + \"val_loss_history.txt\", np_vloss, delimiter=\",\")\n",
        "  \n",
        "  # ã‚°ãƒ©ãƒ•ã®ä¿å­˜\n",
        "  fig = plot_history_loss(np_loss, np_vloss, loss.__name__)\n",
        "  fig.savefig(file_prefix + \"loss_history.png\")\n",
        "  plt.close()\n",
        "  \n",
        "  # å­¦ç¿’ã—ãŸé‡ã¿ã‚’ä¿å­˜\n",
        "  autoencoder.save_weights(file_prefix + 'autoencoder.h5')\n",
        "  \n",
        "  autoencoder.reset_states()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n",
            "start: mean_squared_error_relu_SGD_\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "start: mean_squared_error_relu_RMSprop_\n",
            "start: mean_squared_error_relu_Adagrad_\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mVwTIp0j2BUE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ------------------------------------------\n",
        "for loss, dact, optimizer in product(lossfs, acts, opzs):\n",
        "  file_prefix = (loss.__name__ + '_' + dact + '_' + optimizer + '_' )\n",
        "  print(\"start: \" + file_prefix )\n",
        "  encoding_dim = 32\n",
        "  input_img = Input(shape=(784,))\n",
        "  encoded = Dense(encoding_dim, activation='relu', \n",
        "                  kernel_constraint=cst.non_neg(), bias_constraint=cst.non_neg())(input_img) \n",
        "  decoded = Dense(784, activation=dact, kernel_constraint=cst.non_neg(), bias_constraint=cst.non_neg())(encoded)\n",
        "  autoencoder = Model(inputs=input_img, outputs=decoded) # https://medium.com/@zhuixiyou/userwarning-update-your-model-call-to-the-keras-2-api-8a6a5955daac\n",
        "  # autoencoderã§ã¯ã€æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã«ãƒ©ãƒ™ãƒ«ã‚’ä½¿ã‚ãªã„ãŸã‚accuracyã®è¨ˆç®—ã¯ä¸è¦ã€‚\n",
        "  autoencoder.compile(optimizer=optimizer, loss=loss)\n",
        "  # ------------------------------------------\n",
        "  # ä¿å­˜ã—ãŸé‡ã¿ã‚’èª­ã¿è¾¼ã¿\n",
        "  autoencoder.load_weights(file_prefix + 'autoencoder.h5')\n",
        "  decoded_imgs = autoencoder.predict(x_test)\n",
        "  # 0-9ã‚’è¡¨ç¤ºã™ã‚‹\n",
        "  plt.figure(figsize=(20, 4))\n",
        "  label = 0\n",
        "  for i in range(1000):\n",
        "      if label > 10:\n",
        "          break\n",
        "      elif label != y_test[i]:\n",
        "          continue\n",
        "      # ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ãƒ†ã‚¹ãƒˆç”»åƒã‚’è¡¨ç¤º\n",
        "      plt.imshow(x_test[i].reshape(28, 28))\n",
        "      plt.gray()\n",
        "      plt.savefig(str(i) + '_x_test_numbers.png')\n",
        "\n",
        "      # å¤‰æ›ã•ã‚ŒãŸç”»åƒã‚’è¡¨ç¤º\n",
        "      plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "      plt.gray()\n",
        "      plt.savefig(file_prefix + str(i) + '_decoded_numbers.png')\n",
        "      \n",
        "      label+=1\n",
        "  #  plt.show() #show()ã™ã‚‹ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã§ããªã„ã“ã¨ã«æ³¨æ„ã€‚\n",
        "  autoencoder.reset_states()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LK23VO2zjflG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ä¿å­˜ã—ãŸé‡ã¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
        "from google.colab import files\n",
        "!zip -r h5.zip *.h5\n",
        "!zip -r png.zip *.png\n",
        "!zip -r txt.zip *.txt\n",
        "files.download('h5.zip')\n",
        "files.download('png.zip')\n",
        "files.download('txt.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_j9zZq4yb4Sc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://www.tensorflow.org/api_docs/python/tf/image/ssim\n",
        "'''\n",
        "This function is based on the standard SSIM implementation from:\n",
        "Wang, Z., Bovik, A. C., Sheikh, H. R., & Simoncelli, E. P. (2004).\n",
        "Image quality assessment: from error visibility to structural similarity. \n",
        "IEEE transactions on image processing\n",
        "'''\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}