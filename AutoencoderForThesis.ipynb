{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoencoderForThesis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s1230038/autoencoder/blob/master/AutoencoderForThesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "d5NaL7in5Jc1",
        "colab_type": "code",
        "outputId": "ad2c013b-cccb-48b5-b0e1-65959d9ef5a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "# Simple Autoencoder using Other Loss Function\n",
        "# Original: https://elix-tech.github.io/ja/2016/07/17/autoencoder.html\n",
        "# https://colab.research.google.com/drive/1Z_d8APkMUDwXDQIg3OI7E13vH8IZhusM?authuser=1#scrollTo=WmBfOis_mWCH\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras import backend as K\n",
        "from keras import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import losses\n",
        "from tensorflow.python import debug as tf_debug\n",
        "from itertools import product\n",
        "from google.colab import files\n",
        "\n",
        "# imititing mean_squared_error():\n",
        "# é’ã‚¤ãƒ«ã‚« P.57ã§ã¯äºŒä¹—èª¤å·®ã®ç·å’Œã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãŒã€ã“ã‚Œã‚’Kerasã§å®Ÿè£…ã™ã‚‹å ´åˆã¯å¹³å‡äºŒä¹—èª¤å·®ï¼ˆmean_squared_error)\n",
        "# ã‚’loss ã«æ¸¡ã™å¿…è¦ãŒã‚ã‚‹ã€‚ãªãœãªã‚‰ã€Kerasã¯ãƒŸãƒ‹ãƒãƒƒãƒã§fit()ã‚’è¨ˆç®—ã™ã‚‹ã‹ã‚‰ã§ã‚ã‚‹ã€‚é’ã‚¤ãƒ«ã‚« P.27å‚ç…§ã€‚\n",
        "# ğ’™ Ì‚_ğ‘› :y_pred,  ğ’™_ğ‘› : y_true, because x_n is training data which means label.\n",
        "def i_divergence1(y_true, y_pred):\n",
        "  y_true = K.clip(y_true, K.epsilon(), 1)\n",
        "  y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
        "  return K.mean(y_pred * K.log(y_pred / y_true) - y_pred + y_true, axis=-1)\n",
        "\n",
        "def i_divergence2(y_true, y_pred):\n",
        "  y_true = K.clip(y_true, K.epsilon(), 1)\n",
        "  y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
        "  return K.mean(y_true * K.log(y_true / y_pred) - y_true + y_pred, axis=-1)\n",
        "\n",
        "def is_divergence1(y_true, y_pred):\n",
        "  y_true = K.clip(y_true, K.epsilon(), 1)\n",
        "  y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
        "  return K.mean( (y_pred / y_true) - K.log(y_pred / y_true) - 1, axis=-1)\n",
        "\n",
        "def is_divergence2(y_true, y_pred):\n",
        "  y_true = K.clip(y_true, K.epsilon(), 1)\n",
        "  y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
        "  return K.mean( (y_true / y_pred) - K.log(y_true / y_pred) - 1, axis=-1)\n",
        "\n",
        "# https://qiita.com/hiroyuki827/items/213146d551a6e2227810\n",
        "def plot_history_loss(np_loss, np_vloss, name):\n",
        "    # Plot the loss in the history\n",
        "    fig, axL = plt.subplots(figsize=(8,6), dpi=500) # ã‚°ãƒ©ãƒ•ã®è¡¨ç¤ºæº–å‚™\n",
        "    axL.plot(np_loss, label=\"loss for training\")\n",
        "    axL.plot(np_vloss, label=\"loss for validation\")\n",
        "    axL.set_title('model loss: ' + name)\n",
        "    axL.set_xlabel('epoch')\n",
        "    axL.set_ylabel('loss')\n",
        "    axL.legend(loc='upper right')\n",
        "    return fig\n",
        "\n",
        "\n",
        "epochs = 50\n",
        "# Download MNIST and standardize, learning\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# we will restrict domain of definition of the input data to the below expression with normalization of the input.\n",
        "x_train = x_train.astype('float32') / 255. # ç”»åƒãƒ‡ãƒ¼ã‚¿ã¯0ã‹ã‚‰1ã®å®Ÿæ•°å€¤ã‚’å–ã‚‹ã‚ˆã†ã«è¦æ ¼åŒ–\n",
        "x_test = x_test.astype('float32') / 255.   # {0,1}ã®äºŒå€¤ã§ã¯ãªãå®Ÿæ•°å€¤ã§ã‚ã‚‹ã“ã¨ã«æ³¨æ„\n",
        "# x_trainã¯ (60000, 28, 28) ã¨ã„ã†å½¢ã‚’ã—ã¦ã„ã¾ã™ãŒã€784æ¬¡å…ƒã®å…¥åŠ›ã«ãªã‚‹ã‚ˆã†ã« (60000, 784) ã«å¤‰å½¢\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "# hyper parameter combination\n",
        "lossfs = [losses.mean_squared_error, i_divergence1, i_divergence2, is_divergence1, is_divergence2]\n",
        "acts = [\"relu\", \"sigmoid\"]\n",
        "opzs = [\"SGD\", \"RMSprop\", \"Adagrad\", \"Adadelta\", \"Adam\", \"Adamax\", \"Nadam\"]\n",
        "for loss, dact, opz in product(lossfs, acts, opzs):\n",
        "  print(\"start: loss = \" + loss.__name__ + \" : dact = \" + dact + \" : opz = \" + opz )\n",
        "  encoding_dim = 32\n",
        "  input_img = Input(shape=(784,))\n",
        "  encoded = Dense(encoding_dim, activation='relu')(input_img) \n",
        "  decoded = Dense(784, activation=dact)(encoded)\n",
        "  autoencoder = Model(inputs=input_img, outputs=decoded) # https://medium.com/@zhuixiyou/userwarning-update-your-model-call-to-the-keras-2-api-8a6a5955daac\n",
        "  # autoencoderã§ã¯ã€æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã«ãƒ©ãƒ™ãƒ«ã‚’ä½¿ã‚ãªã„ãŸã‚accuracyã®è¨ˆç®—ã¯ä¸è¦ã€‚\n",
        "  autoencoder.compile(optimizer=opz, loss=loss)\n",
        "\n",
        "  fit = autoencoder.fit(x_train, x_train,\n",
        "                  epochs=epochs,\n",
        "                  batch_size=256,\n",
        "                  shuffle=True,\n",
        "                  verbose=0,\n",
        "                  validation_data=(x_test, x_test))\n",
        "\n",
        "  file_prefix = loss.__name__ + '_' + dact + '_' + opz + '_'\n",
        "  # lossã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜\n",
        "  loss_his = fit.history['loss']\n",
        "  vloss_his = fit.history['val_loss']\n",
        "  np_loss = np.array(loss_his)\n",
        "  np_vloss = np.array(vloss_his)\n",
        "  np.savetxt(file_prefix + \"loss_history.txt\",     np_loss,  delimiter=\",\")\n",
        "  np.savetxt(file_prefix + \"val_loss_history.txt\", np_vloss, delimiter=\",\")\n",
        "  \n",
        "  # ã‚°ãƒ©ãƒ•ã®ä¿å­˜\n",
        "  fig = plot_history_loss(np_loss, np_vloss, loss.__name__)\n",
        "  fig.savefig(file_prefix + \"loss_history.png\")\n",
        "  plt.close()\n",
        "  \n",
        "  # å­¦ç¿’ã—ãŸé‡ã¿ã‚’ä¿å­˜\n",
        "  autoencoder.save_weights(file_prefix + 'autoencoder.h5')\n",
        "  \n",
        "  autoencoder.reset_states()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "start: loss = mean_squared_error : dact = relu : opz = SGD\n",
            "start: loss = mean_squared_error : dact = relu : opz = RMSprop\n",
            "start: loss = mean_squared_error : dact = relu : opz = Adagrad\n",
            "start: loss = mean_squared_error : dact = relu : opz = Adadelta\n",
            "start: loss = mean_squared_error : dact = relu : opz = Adam\n",
            "start: loss = mean_squared_error : dact = relu : opz = Adamax\n",
            "start: loss = mean_squared_error : dact = relu : opz = Nadam\n",
            "start: loss = mean_squared_error : dact = sigmoid : opz = SGD\n",
            "start: loss = mean_squared_error : dact = sigmoid : opz = RMSprop\n",
            "start: loss = mean_squared_error : dact = sigmoid : opz = Adagrad\n",
            "start: loss = mean_squared_error : dact = sigmoid : opz = Adadelta\n",
            "start: loss = mean_squared_error : dact = sigmoid : opz = Adam\n",
            "start: loss = mean_squared_error : dact = sigmoid : opz = Adamax\n",
            "start: loss = mean_squared_error : dact = sigmoid : opz = Nadam\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mVwTIp0j2BUE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# lossfs = [losses.mean_squared_error, i_divergence1, i_divergence2, is_divergence1, is_divergence2]\n",
        "for loss, dact, opz in product(lossfs, acts, opzs):\n",
        "  print(\"show: \" + loss.__name__ + '_' + dact + '_' + opz)\n",
        "  file_prefix = loss.__name__ + '_' + dact + '_' + opz + '_'\n",
        "  # ãƒ¢ãƒ‡ãƒ«ã®å†ä½œæˆ\n",
        "  encoding_dim = 32\n",
        "  input_img = Input(shape=(784,))\n",
        "  encoded = Dense(encoding_dim, activation='relu')(input_img) \n",
        "  decoded = Dense(784, activation=dact)(encoded)\n",
        "  autoencoder = Model(inputs=input_img, outputs=decoded)\n",
        "  autoencoder.compile(optimizer=opz, loss=loss)\n",
        "  # ä¿å­˜ã—ãŸé‡ã¿ã‚’èª­ã¿è¾¼ã¿\n",
        "  autoencoder.load_weights(file_prefix + 'autoencoder.h5')\n",
        "  decoded_imgs = autoencoder.predict(x_test)\n",
        "  # 0-9ã‚’è¡¨ç¤ºã™ã‚‹\n",
        "  plt.figure(figsize=(20, 4))\n",
        "  label = 0\n",
        "  for i in range(1000):\n",
        "      if label > 10:\n",
        "          break\n",
        "      elif label != y_test[i]:\n",
        "          continue\n",
        "      # ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ãƒ†ã‚¹ãƒˆç”»åƒã‚’è¡¨ç¤º\n",
        "      ax = plt.subplot(2, 10, label+1)\n",
        "      plt.imshow(x_test[i].reshape(28, 28))\n",
        "      plt.gray()\n",
        "      ax.get_xaxis().set_visible(False)\n",
        "      ax.get_yaxis().set_visible(False)\n",
        "\n",
        "      # å¤‰æ›ã•ã‚ŒãŸç”»åƒã‚’è¡¨ç¤º\n",
        "      ax = plt.subplot(2, 10, label+1+10)\n",
        "      plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "      plt.gray()\n",
        "      ax.get_xaxis().set_visible(False)\n",
        "      ax.get_yaxis().set_visible(False)\n",
        "      \n",
        "      label+=1\n",
        "  #  plt.show() #show()ã™ã‚‹ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã§ããªã„ã“ã¨ã«æ³¨æ„ã€‚\n",
        "  plt.savefig(file_prefix + 'numbers.png')\n",
        "  autoencoder.reset_states()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LK23VO2zjflG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ä¿å­˜ã—ãŸé‡ã¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
        "from google.colab import files\n",
        "!zip -r h5.zip *.h5\n",
        "!zip -r png.zip *.png\n",
        "!zip -r txt.zip *.txt\n",
        "files.download('h5.zip')\n",
        "files.download('png.zip')\n",
        "files.download('txt.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4kBoEeqJmlkL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for loss, dact, opz in product(lossfs, acts, opzs):\n",
        "  print(\"show: \" + loss.__name__ + '_' + dact + '_' + opz)\n",
        "  # ãƒ¢ãƒ‡ãƒ«ã®å†ä½œæˆ\n",
        "  encoding_dim = 32\n",
        "  input_img = Input(shape=(784,))\n",
        "  encoded = Dense(encoding_dim, activation='relu')(input_img) \n",
        "  decoded = Dense(784, activation=dact)(encoded)\n",
        "  autoencoder = Model(inputs=input_img, outputs=decoded)\n",
        "  autoencoder.compile(optimizer=opz, loss=loss)\n",
        "  # ä¿å­˜ã—ãŸé‡ã¿ã‚’èª­ã¿è¾¼ã¿\n",
        "  autoencoder.load_weights(loss.__name__ + '_' + dact + '_' + opz + '_' + 'autoencoder.h5')\n",
        "  decoded_imgs = autoencoder.predict(x_test)\n",
        "  # ä½•å€‹è¡¨ç¤ºã™ã‚‹ã‹\n",
        "  n = 20\n",
        "  plt.figure(figsize=(20, 4))\n",
        "  for i in range(n):\n",
        "      # ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ãƒ†ã‚¹ãƒˆç”»åƒã‚’è¡¨ç¤º\n",
        "      ax = plt.subplot(2, n, i+1)\n",
        "      plt.imshow(x_test[i].reshape(28, 28))\n",
        "      plt.gray()\n",
        "      ax.get_xaxis().set_visible(False)\n",
        "      ax.get_yaxis().set_visible(False)\n",
        "\n",
        "      # å¤‰æ›ã•ã‚ŒãŸç”»åƒã‚’è¡¨ç¤º\n",
        "      ax = plt.subplot(2, n, i+1+n)\n",
        "      plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "      plt.gray()\n",
        "      ax.get_xaxis().set_visible(False)\n",
        "      ax.get_yaxis().set_visible(False)\n",
        "  plt.show()\n",
        "  autoencoder.reset_states()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XfTHq9V2i8qo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !rm *txt *h5 *png *zip"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}